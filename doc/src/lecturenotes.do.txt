TITLE: Using Machine Learning to Classify Phase Transitions
AUTHOR: Morten Hjorth-Jensen at Department of Physics and Center for Computing in Science Education, University of Oslo, Norway
DATE: CEACM Flagship School: Machine Learning in Physical Sciences: Theory and Applications, May 26-30, 2025


!split
===== What is this about? =====
!bblock
These notes, with pertinent exercises cover the following topics. 
o Phase Transitions & Critical Phenomena: Definitions and key concepts (order parameters, critical points, first vs second order).
o Spin Models: 2D Ising model and the q-state Potts model (examples of phase transitions).
o Data Generation: Monte Carlo simulations for sampling spin configurations across temperatures.
o Unsupervised Learning (PCA): Principal Component Analysis to visualize phase separation without labels.
o Supervised Learning (CNN): Convolutional Neural Networks for classifying phases from raw configurations.
o Generative Models (VAE): Variational Autoencoders for latent representation learning and critical anomaly detection.
o Comparisons: Interpretability and performance trade-offs between PCA, CNN, and VAE.
!eblock

!split
===== Where do I find the material? =====

All the material here can be found in the PDF files, codes and jupyter-notebooks at the above _doc_ folder, see the _pub_ subfolder, link to be added  


!split
=====  AI/ML and some statements you may have heard (and what do they mean?)  =====

o Fei-Fei Li on ImageNet: _map out the entire world of objects_ ("The data that transformed AI research":"https://cacm.acm.org/news/219702-the-data-that-transformed-ai-research-and-possibly-the-world/fulltext")
o Russell and Norvig in their popular textbook: _relevant to any intellectual task; it is truly a universal field_ ("Artificial Intelligence, A modern approach":"http://aima.cs.berkeley.edu/")
o Woody Bledsoe puts it more bluntly: _in the long run, AI is the only science_ (quoted in Pamilla McCorduck, "Machines who think":"https://www.pamelamccorduck.com/machines-who-think")


If you wish to have a critical read on AI/ML from a societal point of view, see "Kate Crawford's recent text Atlas of AI":"https://www.katecrawford.net/". 

_Here: with AI/ML we intend a collection of machine learning methods with an emphasis on statistical learning and data analysis_



!split
===== Types of machine learning =====

!bblock
The approaches to machine learning are many, but are often split into two main categories. 
In *supervised learning* we know the answer to a problem,
and let the computer deduce the logic behind it. On the other hand, *unsupervised learning*
is a method for finding patterns and relationship in data sets without any prior knowledge of the system.

An important  third category is  *reinforcement learning*. This is a paradigm 
of learning inspired by behavioural psychology, where learning is achieved by trial-and-error, 
solely from rewards and punishment.
!eblock

!split
===== Main categories =====
!bblock
Another way to categorize machine learning tasks is to consider the desired output of a system.
Some of the most common tasks are:

  * Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.

  * Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.

  * Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.
!eblock



!split
=====  The plethora  of machine learning algorithms/methods =====

o Deep learning: Neural Networks (NN), Convolutional NN, Recurrent NN, Boltzmann machines, autoencoders and variational autoencoders  and generative adversarial networks, stable diffusion and many more generative models
o Bayesian statistics and Bayesian Machine Learning, Bayesian experimental design, Bayesian Regression models, Bayesian neural networks, Gaussian processes and much more
o Dimensionality reduction (Principal component analysis), Clustering Methods and more
o Ensemble Methods, Random forests, bagging and voting methods, gradient boosting approaches 
o Linear and logistic regression, Kernel methods, support vector machines and more
o Reinforcement Learning; Transfer Learning and more 



!split
===== Example of generative modeling, "taken from Generative Deep Learning by David Foster":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"  =====

FIGURE: [figures/generativelearning.png, width=900 frac=1.0]


!split
===== Example of discriminative modeling, "taken from Generative Deeep Learning by David Foster":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"  =====


FIGURE: [figures/standarddeeplearning.png, width=900 frac=1.0]



!split
===== Taxonomy of generative deep learning, "taken from Generative Deep Learning by David Foster":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"  =====

FIGURE: [figures/generativemodels.png, width=900 frac=1.0]


!split
===== Good books with hands-on material and codes =====
!bblock
* "Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch":"https://sebastianraschka.com/blog/2022/ml-pytorch-book.html"
* "David Foster, Generative Deep Learning with TensorFlow":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"
* "Bali and Gavras, Generative AI with Python and TensorFlow 2":"https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2"
!eblock

All three books have GitHub addresses from where  one can download all codes. We will borrow most of the material from these three texts as well as 
from Goodfellow, Bengio and Courville's text "Deep Learning":"https://www.deeplearningbook.org/"




!split
===== What are the basic Machine Learning ingredients? =====
!bblock
Almost every problem in ML and data science starts with the same ingredients:
* The dataset $\bm{x}$ (could be some observable quantity of the system we are studying)
* A model which is a function of a set of parameters $\bm{\alpha}$ that relates to the dataset, say a likelihood  function $p(\bm{x}\vert \bm{\alpha})$ or just a simple model $f(\bm{\alpha})$
* A so-called _loss/cost/risk_ function $\mathcal{C} (\bm{x}, f(\bm{\alpha}))$ which allows us to decide how well our model represents the dataset. 

We seek to minimize the function $\mathcal{C} (\bm{x}, f(\bm{\alpha}))$ by finding the parameter values which minimize $\mathcal{C}$. This leads to  various minimization algorithms. It may surprise many, but at the heart of all machine learning algortihms there is an optimization problem. 
!eblock

!split
===== Low-level machine learning, the family of ordinary least squares methods  =====

Our data which we want to apply a machine learning method on, consist
of a set of inputs $\bm{x}^T=[x_0,x_1,x_2,\dots,x_{n-1}]$ and the
outputs we want to model $\bm{y}^T=[y_0,y_1,y_2,\dots,y_{n-1}]$.
We assume  that the output data can be represented (for a regression case) by a continuous function $f$
through
!bt
\[
\bm{y}=f(\bm{x})+\bm{\epsilon}.
\]
!et

!split
===== Setting up the equations =====

In linear regression we approximate the unknown function with another
continuous function $\tilde{\bm{y}}(\bm{x})$ which depends linearly on
some unknown parameters
$\bm{\theta}^T=[\theta_0,\theta_1,\theta_2,\dots,\theta_{p-1}]$.

The input data can be organized in terms of a so-called design matrix 
with an approximating function $\bm{\tilde{y}}$ 
!bt
\[
\bm{\tilde{y}}= \bm{X}\bm{\theta},
\]
!et


!split
===== The objective/cost/loss function =====

The  simplest approach is the mean squared error
!bt
\[
C(\bm{\Theta})=\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\left\{\left(\bm{y}-\bm{\tilde{y}}\right)^T\left(\bm{y}-\bm{\tilde{y}}\right)\right\},
\]
!et
or using the matrix $\bm{X}$ and in a more compact matrix-vector notation as
!bt
\[
C(\bm{\Theta})=\frac{1}{n}\left\{\left(\bm{y}-\bm{X}\bm{\theta}\right)^T\left(\bm{y}-\bm{X}\bm{\theta}\right)\right\}.
\]
!et
This function represents one of many possible ways to define the so-called cost function.


!split
===== Training solution  =====

Optimizing with respect to the unknown parameters $\theta_j$ we get 
!bt
\[
\bm{X}^T\bm{y} = \bm{X}^T\bm{X}\bm{\theta},  
\]
!et
and if the matrix $\bm{X}^T\bm{X}$ is invertible we have the optimal values
!bt
\[
\hat{\bm{\theta}} =\left(\bm{X}^T\bm{X}\right)^{-1}\bm{X}^T\bm{y}.
\]
!et

We say we 'learn' the unknown parameters $\bm{\theta}$ from the last equation.




!split
===== Selected references =====
!bblock
* "Mehta et al.":"https://arxiv.org/abs/1803.08823" and "Physics Reports (2019)":"https://www.sciencedirect.com/science/article/pii/S0370157319300766?via%3Dihub".
* "Machine Learning and the Physical Sciences by Carleo et al":"https://link.aps.org/doi/10.1103/RevModPhys.91.045002"
* "Artificial Intelligence and Machine Learning in Nuclear Physics, Amber Boehnlein et al., Reviews Modern of Physics 94, 031003 (2022)":"https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.94.031003" 
* "Dilute neutron star matter from neural-network quantum states by Fore et al, Physical Review Research 5, 033062 (2023)":"https://journals.aps.org/prresearch/pdf/10.1103/PhysRevResearch.5.033062"
* "Neural-network quantum states for ultra-cold Fermi gases, Jane Kim et al, Nature Physics Communcication, submitted":"https://doi.org/10.48550/arXiv.2305.08831"
* "Message-Passing Neural Quantum States for the Homogeneous Electron Gas, Gabriel Pescia, Jane Kim et al. arXiv.2305.07240,":"https://doi.org/10.48550/arXiv.2305.07240"
* "Efficient solutions of fermionic systems using artificial neural networks, Nordhagen et al, Frontiers in Physics
!eblock





!split
===== Machine learning. A simple perspective on the interface between ML and Physics =====

FIGURE: [figures/mlimage.png, width=800 frac=1.0]


!split
===== ML in Nuclear  Physics (or any field in physics) =====

FIGURE: [figures/ML-NP.pdf, width=900 frac=1.0]




!split
===== Phase Transitions and Critical Phenomena =====

o Definition: A phase transition is characterized by an abrupt, non-analytic change in a macroscopic property of a system as some external parameter (e.g. temperature) is varied . In simpler terms, the system’s state or phase changes dramatically at a critical point.
o Order Parameter: Associated with each phase transition is an order parameter – a quantity that is zero in one phase and non-zero in the other. For example, magnetization plays the role of an order parameter in magnetic systems, distinguishing ordered (magnetized) from disordered (unmagnetized) phases.
o Critical Point: At the critical temperature (or pressure, etc.), the order parameter changes (continuous or discontinuous) and the system exhibits critical phenomena: large fluctuations, divergence of correlation length, and the onset of scale invariance. Critical points of second-order transitions feature continuous change of the order parameter with characteristic critical exponents and universal behavior across different systems.




!split
===== Phase Transitions: Definitions =====
o A *phase transition* is a qualitative change in the state of a system when a control parameter (e.g.\ temperature) passes a critical point.
o _Order parameter_: quantity that distinguishes phases (e.g.\ magnetization $M$ for magnetic systems).
o _Order vs disorder_: e.g.\ below $T_c$ a ferromagnet has $|M|>0$ (ordered), above $T_c$ $M=0$ (disordered).
o Phases can break symmetries; transitions can be \emph{continuous} (second-order) or \emph{first-order}.

!split
===== Order Parameter and Symmetry Breaking =====
o Phase transitions often involve spontaneous symmetry breaking (e.g.\ Ising model $Z_2$ symmetry).
o The order parameter (e.g.\ magnetization $M = \frac{1}{N}\sum_i s_i$) changes behavior at $T_c$.
o In ferromagnets: $M=0$ for $T>T_c$ (symmetric paramagnet), $M\neq 0$ for $T<T_c$ (broken symmetry).
o Example: in 2D Ising model, two symmetric ordered states (up/down) below $T_c$.


!split
===== Phase Transitions: Overview =====


o _Definition_: A phase transition is characterized by an abrupt, non-analytic change in a macroscopic property of a system as some external parameter (e.g. temperature) is varied . In simpler terms, the system’s state or phase changes dramatically at a critical point.
o _Order Parameter_: Associated with each phase transition is an order parameter – a quantity that is zero in one phase and non-zero in the other. For example, magnetization plays the role of an order parameter in magnetic systems, distinguishing ordered (magnetized) from disordered (unmagnetized) phases.
o _Critical Point_: At the critical temperature (or pressure, etc.), the order parameter changes (continuous or discontinuous) and the system exhibits critical phenomena: large fluctuations, divergence of correlation length, and the onset of scale invariance. Critical points of second-order transitions feature continuous change of the order parameter with characteristic critical exponents and universal behavior across different systems.





!split
===== Phase Transitions: First vs Second Order =====


o _First-Order vs Second-Order_: In a first-order transition, the order parameter changes discontinuously at the transition (often with latent heat), whereas in a second-order (continuous) transition, the order parameter goes to zero continuously at $T_c$, accompanied by diverging susceptibility and correlation length. For example, the liquid–gas transition (at sub-critical pressures) is first-order, while the ferromagnetic transition in the 2D Ising model is second-order (continuous).
o _Example, Potts Model Transitions_: The q-state Potts model generalizes Ising (which is q=2). In 2D, the Potts model undergoes a continuous transition for $q \le  4$ and a discontinuous (first-order) transition for $q>4$ . This highlights how the nature of the phase transition can change with system parameters.
o _Critical Phenomena_: Near second-order transitions, critical phenomena include power-law divergences (e.g. specific heat, susceptibility), critical opalescence (fluctuations at all scales), and universality (different systems share the same critical exponents if they have the same symmetry and dimensionality). These concepts set the stage for identifying phase transitions through data features (e.g. large fluctuations near $T_c# might be detectable by learning algorithms).




!split
===== Critical Phenomena and Scaling =====

o Near a continuous transition, observables follow power laws: $M \sim |T-T_c|^\beta$, correlation length $\xi \sim |T-T_c|^{-\nu}$, etc.
o _Critical exponents_ ($\alpha,\beta,\gamma,\nu,\dots$) characterize singular behavior.
o Universality: systems with the same symmetry and dimension share exponents.
o The classical example is the  two-dimensional  Ising exponents known analytically (Onsager).
o At $T\to T_c$, correlation length $\xi\to\infty$, large-scale fluctuations appear.

!split
===== 2D Ising Model: Definition =====
o Spins $s_i = \pm 1$ on a 2D square lattice, nearest-neighbor ferromagnetic coupling.
o Hamiltonian: $H = -J \sum_{\langle i,j\rangle} s_i s_j,$ with $J>0$ favoring alignment.
o Exhibits a second-order phase transition at critical temperature $T_c$.
o Order parameter: magnetization $M = \frac{1}{N}\sum_i s_i$.
o Below $T_c$, $M\neq0$ (ferromagnetic order); above $T_c$, $M=0$ (paramagnet).

!split
===== 2D Ising Model: Critical Temperature =====
o Exact result (Onsager): critical point $T_c$ satisfies $T_c \approx \frac{2J}{\ln(1+\sqrt{2})}\approx 2.269J$.
o At $T>T_c$: spins are mostly disordered, no net magnetization.
o At $T<T_c$: long-range order develops (nonzero $M$).
o Correlation length $\xi$ diverges at $T_c$ 
o Example: at $T=T_c$ large clusters of aligned spins appear.

!split
===== q-State Potts Model: Definition =====
* Generalization of Ising: each spin $s_i \in \{1,2,\dots,q\}$.
* Ferromagnetic Potts Hamiltonian:
!bt
     \[
       H = -J \sum_{\langle i,j\rangle} \delta_{s_i,s_j},
     \]
!et     
where $\delta_{a,b}=1$ if $a=b$, else $0$.
* If $q=2$, reduces to the Ising model. Higher $q$ allows richer symmetry breaking ($\mathbb{Z}_q$).
* Widely used to study phase transitions with multiple equivalent ordered states.

!split
===== 2D Potts Model: Phase Behavior =====
* In 2D, the ferromagnetic Potts model has a phase transition for all $q\ge1$ 
* Exact critical point:
!bt
     \[
       \frac{J}{k_B T_c} = \ln\!\bigl(1+\sqrt{q}\bigr).
     \]
!et
* The nature of the transition depends on $q$ 
  * $1 \le q \le 4$: continuous (second-order) transition.
  * $q > 4$: discontinuous (first-order) transition (latent heat appears).
* Example: $q=3,4$ have continuous transitions; $q=5$ and higher show first-order behavior.

!split
===== Monte Carlo Sampling of Spin Models =====
o Use Monte Carlo (MC) to generate spin configurations at given $T$: sample from Boltzmann distribution $P\propto e^{-H/T}$.
o Metropolis algorithm: attempt random single-spin flips to equilibrate the system.
o Provides training data: spin configurations $\{s_i\}$ labeled by temperature or phase.
o Ensures statistical equilibrium and detailed balance 
o Efficient sampling (especially near $T_c$ cluster algorithms help, e.g.\ Wolff or Swendsen-Wang).

!split
===== Metropolis Algorithm =====
* Initialize spins randomly or in a fixed state.
* Repeat for many steps:
  o Pick a random lattice site $i$.
  o Propose flipping $s_i \to -s_i$ (Ising) or change state (Potts).
  o Compute energy change $\Delta E$.
  o If $\Delta E \le 0$, accept the flip (lower energy).
  o Else accept with probability $\exp(-\Delta E/T)$ (Boltzmann factor) 
  o Otherwise, reject and keep the old state.
* After equilibration, record configurations as samples.

!split
===== Metropolis Algorithm (Pseudo-code) =====
!bc pycod
for T in temperature_list:
   # Initialize lattice (e.g., random spins)
   config = random_configuration(Lx, Ly)
   for step in range(num_steps):
       i,j = random_site()
       dE = compute_deltaE(config, i, j)  # energy change if spin flipped
       if dE <= 0 or rand() < exp(-dE/T):
           flip_spin(config, i, j)
   record_configuration(config, T)
!ec

!split
===== Monte Carlo Data for ML =====
o Generate many spin configurations across a range of temperatures $T$.
o Label each configuration by its temperature or by phase (ordered/disordered).
o This labeled dataset is used for \emph{supervised} methods (e.g.\ CNN).
o For \emph{unsupervised} methods (PCA, VAE), labels are not used in training.
o Data augmentation: one can use symmetries (e.g.\ spin flip) to enlarge dataset.


!split
===== Principal Component Analysis (PCA) Basics =====

o PCA is an unsupervised method for dimensionality reduction.
o Finds orthogonal directions (principal components) of maximum variance in data.
o Project data onto the first few PCs to visualize structure.
o Advantages: linear, fast, and interpretable (PCs are linear combinations of features).
o Disadvantage: only captures linear correlations (may miss complex features).

!split
===== PCA for Phase Identification =====
o Apply PCA to the ensemble of spin configurations (flattened to vectors).
o The first principal component (PC1) often correlates with the order parameter (e.g.\ magnetization).
o Hu et al. (2017) found PCA distinguishes different phases and can locate critical points 
o By plotting data in the subspace of PCs, one sees separation of low-$T$ (ordered) vs high-$T$ (disordered) points.
o No labels needed: phase transitions are revealed by clustering in PC space 


!split
===== PCA Workflow for Spin Data =====

o Collect data matrix $X$ of shape (num\_samples) $\times$ (num\_features), e.g. $N\times (L\times L)$.
o Subtract the mean from each column (feature) of $X$.
o Compute covariance matrix $C = X^T X$ (or use SVD on $X$ directly).
o Obtain eigenvalues/vectors of $C$: $C = U \Lambda U^T$. Columns of $U$ are principal directions.
o Sort by eigenvalues (variance). Project $X$ onto top $k$ PCs: $X_{\rm red} = X\,U[:,1:k]$.
o Analyze $X_{\rm red}$: e.g. scatter plot PC1 vs PC2.


!split
===== PCA Example: Ising Model =====

o In the 2D Ising model, PC1 is essentially proportional to the overall magnetization.
o At $T<T_c$, configurations cluster with large positive or negative PC1 (ordered states).
o At $T>T_c$, configurations cluster near $PC1 \approx 0$ (disordered).
o The variance captured by PC1 drops sharply at $T_c$, signaling the transition.
o PCA automatically finds these features, without knowing the physics a priori.

!split
===== PCA Limitations =====
o PCA is linear: complex nonlinear features (e.g.\ vortex order) may not be captured.
o Example: In a frustrated 2D spin model, PCA failed to detect certain correlations (vorticity) 
o PCA does not directly classify; it provides features for clustering or visualization.
o Sensitive to scaling: data should be normalized appropriately.
o Still useful as a first-pass: identifies the most significant variations 

!split
===== PCA with PyTorch (Example Code) =====
!bc pycod
import torch

# X: tensor of shape (N, L*L) containing spin configurations as floats (e.g. +1/-1)
# Center the data
X = X - X.mean(dim=0, keepdim=True)

# Compute covariance (or use torch.pca_lowrank)
cov = torch.mm(X.t(), X) / (X.size(0)-1)

# Eigen-decomposition (SVD) of covariance
U, S, V = torch.svd(cov)

# Select first k principal components
k = 2
PCs = U[:, :k]  # shape (L*L, k)

# Project data onto principal components
X_reduced = torch.mm(X, PCs)  # shape (N, k)
!ec

!split
===== Convolutional Neural Networks (CNNs) =====

o CNNs are deep neural networks designed for spatial data (e.g. images).
o Architecture: convolutional layers (feature detectors) + pooling, followed by fully connected layers.
o In physics: treat spin lattice as an image with multiple channels (e.g.\ one channel of spins).
o CNNs can learn complex nonlinear features automatically from data.
o They require labeled examples for training (supervised learning).

!split
===== CNN for Phase Classification =====
o Prepare training data: spin configurations labeled by phase or temperature.
o CNN learns to map configuration $\to$ phase label (ordered/disordered) or predict $T$.
o As shown by Carrasquilla and Melko (2017), CNNs can identify phases from raw states 
o Achieves high accuracy on Ising and other models when training labels are available.
o CNNs exploit locality: can detect clusters or domains of aligned spins via convolution filters.

!split
===== Example CNN Architecture =====
o _Input_: single-channel $L\times L$ lattice (values $-1$ or $+1$).
o _Conv layer 1_: e.g.\ 8 filters of size $3\times3$, ReLU activation, stride=1, padding=1.
o _Conv layer 2_: 16 filters of size $3\times3$, ReLU, followed by a $2\times2$ max-pooling.
o _Fully Connected_: flatten feature maps to vector; FC layer to 64 units (ReLU); final FC to 2 outputs (softmax for binary phase).
o _Training_: minimize cross-entropy loss between predicted and true labels.
o _Note_: architecture and hyperparameters can be tuned for best performance.

!split
===== CNN: Training and Results =====
o Train on many labeled samples (e.g.\ temperatures $T$ and whether $T<T_c$ or $T>T_c$).
o The network learns features such as magnetization domains, energy patterns, etc.
o CNN accuracy can be very high (often $\sim$100\% on clean data) for distinguishing phases.
o Fukushima and Sakai (2021): a CNN trained on 2D Ising can detect transition in $q$-state Potts 
o CNN behavior: at high $T$ it effectively uses average energy; at low $T$ it correlates with magnetization 

!split
===== CNN Interpretability =====
o CNNs are often seen as _black boxes_, but their learned filters can sometimes be interpreted.
o Outputs correlate with known physics:
  o At low $T$: classification heavily influenced by magnetization (order).
  o At high $T$: classification influenced by internal energy (disorder) 
o CNNs can generalize: e.g.\ Ising-trained CNN finds Potts $T_c$ 
o Visualization methods (e.g.\ saliency maps) can highlight what CNN focuses on.

!split
===== CNN (PyTorch) Code Example =====
!bc pycod
import torch
import torch.nn as nn
import torch.nn.functional as F

class PhaseCNN(nn.Module):
   def __init__(self, L):
       super(PhaseCNN, self).__init__()
       self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)  # 1 channel -> 8
       self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1) # 8 -> 16
       self.pool = nn.MaxPool2d(2)  # downsample by 2
       self.fc1 = nn.Linear(16 * (L//2) * (L//2), 64)
       self.fc2 = nn.Linear(64, 2)  # 2 output classes

   def forward(self, x):
       x = F.relu(self.conv1(x))      # (B,8,L,L)
       x = self.pool(F.relu(self.conv2(x)))  # (B,16,L/2,L/2)
       x = x.view(x.size(0), -1)      # flatten
       x = F.relu(self.fc1(x))
       x = self.fc2(x)               # logits for 2 classes
       return x

# Example usage:
model = PhaseCNN(L=32)           # for a 32x32 lattice
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
!ec

!split
===== Variational Autoencoders (VAE) Overview =====

o A VAE is an \emph{unsupervised} generative model that learns a latent representation of data.
o Components:
   o _Encoder_: maps input $X$ to parameters $(\mu, \log\sigma^2)$ of a latent Gaussian.
   o _Latent_ $z$: sampled via $z = \mu + \sigma \epsilon$ ($\epsilon\sim N(0,I)$).
   o _Decoder_: reconstructs input $\hat{X}$ from $z$.
o Loss: reconstruction error + KL divergence to enforce latent prior $\mathcal{N}(0,I)$.
o VAEs can both encode data compactly and generate new samples by sampling $z$.

!split
===== VAE for Spin Configurations =====

o Train VAE on spin configurations (no labels).
o Latent space (usually low-dimensional) captures key features (like order parameter).
o Walker et al. (2020): latent variables provide metrics to track order vs disorder in Ising 
o They found the latent representation closely corresponds to physical order (magnetization) 
o After training, one can:
  o Inspect latent space (e.g.\ scatter plot of $(\mu_1,\mu_2)$) to distinguish phases.
  o Sample $z\sim N(0,1)$ and decode to generate synthetic configurations.

!split
===== VAE Architecture Details =====

* Typically use convolutional encoder/decoder for 2D structure.
* Example:
  o Encoder: conv layers downsampling to a flat vector $\rightarrow$ linear layers $\rightarrow (\mu, \log\sigma^2)$ (size of latent space, e.g.\ 2–10 dims).
  o Decoder: linear layer from $z$ to feature map size, followed by transposed-conv layers to reconstruct $L\times L$ lattice.
* Activation: ReLU (or LeakyReLU); final output often sigmoid to model spin distribution.
* Training with minibatch gradient descent optimizing
!bt
     \[
       \mathcal{L} = \mathbb{E}[\|X - \hat{X}\|^2] +
       \mathrm{KL}(\mathcal{N}(\mu,\sigma)\,\|\,\mathcal{N}(0,1)).
     \]
!et

!split
===== VAE Results on Ising Model =====

o The first latent dimension ($\nu_0$) learned by the VAE correlated strongly with magnetization 
o Plotting $\nu_0$ vs temperature shows clear change around $T_c$ (order–disorder).
o This means VAE "discovered" the order parameter without supervision.
o The VAE predicted the critical region and crossover consistently with theory 
o Latent space clustering: ordered-phase points separate from disordered.


!split
===== VAE: Generation and Interpretation =====

o After training, sample random $z$ from Gaussian prior and decode to generate configurations.
o The VAE latent space is continuous: can interpolate between phases.
o The learned representation is smooth and disentangled: one latent coordinate tracks magnetization, others track disorder.
o VAEs can also be used for anomaly detection: points with unusual $z$ indicate atypical states.
o Overall, VAEs provide both a dimensionally-reduced view of phase structure and a generative model.

!split
===== VAE (PyTorch) Code Example =====
!bc pycod
import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
   def __init__(self, L, latent_dim=2):
       super(VAE, self).__init__()
       # Encoder: conv -> conv -> flatten -> fc_mu/fc_logvar
       self.encoder = nn.Sequential(
           nn.Conv2d(1, 8, 3, stride=2, padding=1),   # -> (8, L/2, L/2)
           nn.ReLU(),
           nn.Conv2d(8, 16, 3, stride=2, padding=1),  # -> (16, L/4, L/4)
           nn.ReLU(),
           nn.Flatten()
       )
       self.fc_mu = nn.Linear(16*(L//4)*(L//4), latent_dim)
       self.fc_logvar = nn.Linear(16*(L//4)*(L//4), latent_dim)

       # Decoder: linear -> unflatten -> convTranspose -> convTranspose
       self.decoder_fc = nn.Linear(latent_dim, 16*(L//4)*(L//4))
       self.decoder = nn.Sequential(
           nn.Unflatten(1, (16, L//4, L//4)),
           nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),  # -> (8, L/2, L/2)
           nn.ReLU(),
           nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1),   # -> (1, L, L)
           nn.Sigmoid()
       )

   def encode(self, x):
       h = self.encoder(x)
       mu = self.fc_mu(h)
       logvar = self.fc_logvar(h)
       return mu, logvar

   def reparameterize(self, mu, logvar):
       std = torch.exp(0.5*logvar)
       eps = torch.randn_like(std)
       return mu + eps * std

   def decode(self, z):
       h = self.decoder_fc(z)
       x_recon = self.decoder(h)
       return x_recon

   def forward(self, x):
       mu, logvar = self.encode(x)
       z = self.reparameterize(mu, logvar)
       x_recon = self.decode(z)
       return x_recon, mu, logvar

# Example instantiation:
model = VAE(L=32, latent_dim=2)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
!ec

!split
===== Supervised vs Unsupervised Methods =====

o _Supervised (CNN)_: Requires labeled data (phase labels or temperatures). Learns a direct mapping $\{\text{config}\}\to\{\text{phase}\}$.
o _Unsupervised (PCA, VAE)_: Uses only the raw configurations without labels. Learns features or representations of the data.
o PCA reduces dimensionality; requires no training labels 
o VAE learns a latent generative model; also label-free 
o CNN typically achieves higher accuracy in classifying known phases, but needs supervised labels.

!split
===== Method Interpretability and Features =====

o _PCA_: Principal components often have clear physical meaning (e.g.\ PC1 and  magnetization) 
o _CNN_: Filters are less directly interpretable; features are learned. However, some correlation with physics (energy, $M$) emerges 
o _VAE_: Latent variables can often be interpreted as order/disorder features (e.g. one latent is approximately equal to the  magnetization) 
o CNN is a "black box" classifier; PCA/VAE provide insight into data structure.
o In terms of visualization: PCA and VAE produce low-dim plots of data (semi-transparent), whereas CNN only outputs a decision boundary.

!split
===== Performance and Use Cases =====

o _PCA_: Fast to compute; good for preliminary analysis of large datasets. Best for linearizable transitions.
o _CNN_: High classification accuracy; powerful for large and complex datasets. Can predict critical $T$ or classify multiple phases 
o _VAE_: Useful when no labels are available; provides a generative model. Effective in detecting transitions by latent statistics 
o Computational cost: PCA very cheap, CNN and VAE require training time (GPU recommended for large data).
o Choosing a method: depends on data availability and goal (classification vs insight vs generation).

!split
===== Summary of Methods =====
o _PCA_: Unsupervised, linear, interpretable. Good for dimensionality reduction and initial exploration
o _CNN_: Supervised, non-linear, high accuracy. Requires labels, but learns complex features (works across models 
o _VAE_: Unsupervised, generative. Learns latent representation reflecting order/disorder 
o Each method has trade-offs in accuracy, interpretability, and data requirements.
o Combining methods (e.g.\ using PCA or VAE features as input to another classifier) can also be fruitful.

!split
===== Conclusions =====
o Machine learning provides powerful tools for studying phase transitions in statistical models.
o *Unsupervised* methods (PCA, VAE) can discover phase structure without labels 
o *Supervised* methods (CNNs) achieve high classification performance given labeled data 
o Interpretability: PCA/VAE offer more insight into physics (latent/PC represent order parameters), while CNNs focus on prediction accuracy.
o Choice of method depends on the problem: data availability, need for generative modeling, and interpretability.
o Future directions: deeper architectures (e.g.\ ResNets), unsupervised generative flows, transfer learning across models, real experimental data.

!split
===== References =====
o Carrasquilla, J. \& Melko, R. G. (2017). Machine learning phases of matter. \emph{Nature Physics}, 13, 431–434 
o Hu, W. \textit{et al.} (2017). Discovering phases, phase transitions through unsupervised ML. \emph{Phys. Rev. E} 95, 062122 
o Fukushima, K. \& Sakai, K. (2021). Can a CNN trained on Ising detect Potts? \emph{Prog. Theor. Exp. Phys.} 2021, 061A01 
o Walker, N. \textit{et al.} (2020). 2D Ising model crossover via VAE. \emph{Sci. Rep.} 10, 13047
o Add refs








2D Ising Model (Ferromagnet)





Figure: Two-dimensional Ising model with spins up (pink arrows) and down (blue arrows) on a square lattice. Each spin can be $s_i = \pm 1$ and interacts with its nearest neighbors; aligned neighbors (same color) contribute a lower energy ($-J$ per bond) while anti-aligned neighbors contribute higher energy ($+J$). The Hamiltonian (with no external field) is $H = -J \sum_{\langle i,j\rangle} s_i s_j$, favoring parallel alignment of spins. The competition between interaction energy and thermal agitation leads to a ferromagnetic phase at low temperatures (most spins align, yielding non-zero net magnetization) and a paramagnetic phase at high temperatures (spins are disordered, zero net magnetization). The 2D square-lattice Ising model is the simplest model that exhibits a phase transition at a finite critical temperature T_c (approximately $T_c \approx 2.269,J/k_B$ for the infinite lattice). Below $T_c$ the system spontaneously magnetizes (symmetry breaking), while above $T_c$ it remains disordered.






q-State Potts Model





Definition: The q-state Potts model is a generalization of the Ising model where each spin can take $q$ discrete values (e.g. $q$ “colors” instead of just up/down). The Hamiltonian for the ferromagnetic Potts model can be written $H = -J \sum_{\langle i,j\rangle} \delta_{\sigma_i,\sigma_j}$, where $\sigma_i \in {1,\dots,q}$ and $\delta$ is the Kronecker delta (neighbor interaction is $-J$ if spins are in the same state). For $q=2$, this reduces to the Ising model.
Phases: Like the Ising case, at low temperature a Potts model magnetically orders (most spins in the same state), and at high temperature it is disordered (spins randomly distributed among the $q$ states). The nature of the transition, however, depends on $q$. In 2D, all $q\ge 1$ have a phase transition at some critical temperature given by $k_B T_c/J = 1/\ln(1+\sqrt{q},)$ . For $q \le 4$ the transition is continuous (second-order), in the same universality class as the Ising model for $q=2$, whereas for $q > 4$ the transition becomes first-order (discontinuous jump in order parameter) .
Significance: The $q$-state Potts model exemplifies how increasing internal symmetry (more spin states) can change transition order. It provides a wider test-bed for machine learning methods: e.g. can an algorithm trained on one type of transition detect another? We will focus primarily on the Ising case (as $q=2$) for data generation and learning, with the understanding that methods can be extended to Potts models and beyond.






Data Generation via Monte Carlo Simulations





Purpose of Simulation: To apply machine learning, we first need data – in this context, spin configuration snapshots at various temperatures. We generate these using Monte Carlo (MC) simulations of the spin models. MC methods (like the Metropolis algorithm or cluster algorithms such as Wolff) allow us to sample representative spin configurations from the equilibrium distribution at a given temperature $T$ (according to the Boltzmann weight $P({s}) \propto e^{-H/k_BT}$).
Metropolis Algorithm: Starting from a random or ordered spin configuration, we randomly flip spins and accept or reject flips based on the energy change $\Delta E$, according to the Metropolis criterion: always accept if $\Delta E \le 0$ (favorable) and accept with probability $e^{-\Delta E/k_BT}$ if $\Delta E > 0$. This procedure ergodically samples the configuration space. We perform many sweeps (updates of all spins) to ensure the system equilibrates at the target temperature before collecting data.
Sampling Configurations: We simulate a range of temperatures spanning below and above the expected $T_c$. For each temperature, we collect many uncorrelated configurations (taking samples sufficiently far apart in MC steps to reduce autocorrelation). These configurations can be represented as 2D images (with spin up vs down as two colors, or values ±1). In practice, researchers generate gigabytes of spin configuration “images” across phases – e.g. hundreds or thousands of configurations at each temperature – to use as the training dataset for machine learning. The labels (phase or temperature) may be attached to each sample if doing supervised learning, or left unlabelled for unsupervised methods.
Example: Carrasquilla and Melko (2017) generated a large collection of Ising model states at various temperatures using simulation, effectively creating a database of “magnet snapshots” on which they trained neural networks . The availability of large simulation datasets is a key enabler for applying modern machine learning to phase transition problems.






Unsupervised Learning: PCA for Phase Separation





Principal Component Analysis: PCA is a classical unsupervised dimensionality reduction technique. It identifies directions (principal components) in feature space along which the data variance is maximal. By projecting high-dimensional data (here, spin configurations with $N$ spins as $N$-dimensional vectors) onto the first few principal components, we obtain a low-dimensional representation that captures the most significant variations in the data.
Applying PCA to Ising Data: We treat each spin configuration as a vector of length $N=L\times L$ (with entries  ±1 for spin-down/up). PCA can be performed on a set of configurations across different temperatures. Interestingly, PCA often finds that the leading principal component corresponds to the order parameter (magnetization per spin) in the Ising model . In other words, the largest variance in the dataset comes from whether the configuration is mostly +1 or mostly –1 (ordered vs disordered spins). This is because below $T_c$ configurations have a bias toward all-up or all-down (high $|M|$), whereas above $T_c$ configurations have $M\approx 0$ on average; thus, when mixed together, the dominant distinguishing feature is the magnetization.
Visualizing Phases: Plotting configurations in the space of the first one or two principal components reveals clusters corresponding to phases. For example, one can observe two clusters of points: low-temperature configurations cluster at extreme values of PC1 (positive or negative, reflecting the two possible magnetization orientations), and high-temperature configurations cluster near PC1 = 0 (no magnetization). This unsupervised clustering means PCA distinguishes the ferromagnetic and paramagnetic phases without any labels, effectively using variance in spin alignment to separate phases . Moreover, by scanning through temperature, one can identify where the data variance (or the separation along PC1) rapidly changes – giving an estimate of the critical temperature. Studies have shown that PCA not only identifies phases but can also locate the transition point and even differentiate types of transitions (e.g. by analyzing how many principal components carry significant variance, one can sometimes tell continuous vs first-order transitions ).
Physical Interpretation: PCA provides an interpretable result: the principal axes (eigenvectors) can often be interpreted in physical terms. For the Ising model, the first principal component’s weight vector is essentially uniform (all spins weighted equally), corresponding to the collective magnetization mode . This aligns with the notion that magnetization is the key feature distinguishing phases. Higher principal components might capture more subtle patterns (e.g. domain wall structures or staggered magnetization if present). The clear mapping of a principal component to an order parameter is a big advantage in interpretability – it tells us the algorithm “learned” a known physical quantity. Of course, PCA is linear and may fail to capture non-linear correlations or more complex orders, but it serves as a powerful baseline.






Example:

 PCA Implementation (PyTorch)



import torch
# X: tensor of shape [num_samples, N_spins] with spin configs (0/1 or ±1)
X = X.float()
X_centered = X - X.mean(dim=0)             # center the data
# Compute PCA via SVD
U, S, V = torch.pca_lowrank(X_centered, q=2)   # get first 2 principal components
PCs = torch.matmul(X_centered, V[:, :2])       # project data onto PC1 and PC2
print(PCs.shape)  # (num_samples, 2)
# PCs can now be used for visualization or clustering of phases
Code: This snippet performs PCA on spin configuration data using PyTorch (utilizing torch.pca_lowrank for efficiency). First we center the data, then obtain the top 2 principal components. The result PCs is a 2D representation of each configuration. Clustering or plotting PCs[:,0] vs PCs[:,1] would show separation between phases (e.g. ordered vs disordered). One could color points by temperature to see the phase transition emerge as a separation in this space.





Supervised Learning: CNN for Phase Classification





Goal: In supervised learning, we provide labeled examples to train a model to classify phases. For the Ising model, a straightforward labeling is to tag each configuration as “ordered” (if $T < T_c$) or “disordered” ($T > T_c$). Alternatively, one can label by temperature value or even try to predict the temperature from the configuration (a regression task). Here we focus on classification into phases using a Convolutional Neural Network (CNN) – a powerful architecture for image recognition tasks.
Why CNN: Spin configurations can be viewed as 2D images (with each site like a pixel of value ±1). A CNN is well-suited to capture local spatial patterns (e.g. clusters of aligned spins, domain walls) via convolution filters. It also respects translational invariance (a domain of up-spins is detected regardless of where it is on the lattice). Earlier work showed that even a simple feed-forward neural network could be trained to identify phases from raw configurations . By using a CNN, we can even capture more complex features, including those relevant for topological phases (which lack a local order parameter) .
Training the CNN: We supply the CNN with many labeled examples of configurations at known temperatures. The network learns to output one class for low-T (ferromagnet) and another for high-T (paramagnet). Remarkably, once trained, the CNN can accurately distinguish an ordered phase from a disordered phase from the raw spin snapshot . When presented with an unseen configuration at an intermediate temperature, the network’s output can indicate the probability of it being in the ordered phase. As one scans temperature, the output probability drops from ~1 to 0 around the critical region, allowing an estimate of $T_c$ (the point of maximal confusion). In Carrasquilla and Melko’s pioneering study, the CNN not only distinguished phases with high accuracy but also identified the phase transition boundary without being told the physics explicitly .
Interpretability: Although CNNs are complex models, we can attempt to interpret what features they learn. In the Ising case, it was found that the trained neural network essentially learned to measure the magnetization of the input configurations . The network’s output was strongly correlated with the magnetization (which is the theoretical order parameter) – indicating the CNN autonomously discovered this key feature to discriminate phases. This is reassuring: the “black box” arrived at a physically meaningful strategy. For more complex phases (e.g. topological phases with no obvious local order parameter), CNNs have been shown to detect transitions as well , though interpreting what they rely on can be more challenging.
Beyond Binary Classification: One can extend this supervised approach to classify multiple phases or phases of the q-state Potts model (with q > 2, there may be more than two phase labels if considering symmetry-broken states as distinct). The network architecture or output layer can be adjusted accordingly (softmax outputs with $n$ classes). Supervised ML has also been used to recognize phases in other models (XY model Kosterlitz-Thouless transition, etc.), sometimes requiring more sophisticated techniques when there is no clear label (there is a method called “learning by confusion” which involves training on hypothetical labels and finding when the network is most confused, pinpointing the transition). Overall, CNNs provide a high-performance tool: with sufficient training data, they can pinpoint phase transitions even in cases where traditional order parameters are not obvious .






Example:

 CNN Architecture & Training (PyTorch)



import torch.nn as nn
import torch.nn.functional as F

class IsingCNN(nn.Module):
    def __init__(self):
        super(IsingCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)   # 1 input channel (spin config), 16 filters
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # 32 filters after second conv
        self.pool  = nn.AdaptiveAvgPool2d(1)  # global average pool to 1x1 output per filter
        self.fc    = nn.Linear(32, 2)         # fully-connected layer for 2 classes (ordered vs disordered)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = self.pool(x)            # result shape: [batch, 32, 1, 1]
        x = torch.flatten(x, 1)     # flatten to [batch, 32]
        x = self.fc(x)              # output logits for 2 classes
        return x

# Initialize model, loss, optimizer
model = IsingCNN()
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# Training loop (simplified)
for epoch in range(10):
    for batch_X, batch_y in train_loader:
        logits = model(batch_X)               # forward pass
        loss = loss_fn(logits, batch_y)       # compute classification loss
        optimizer.zero_grad()
        loss.backward()                       # backpropagate
        optimizer.step()                      # update weights
Code: We define a simple CNN with two convolutional layers and a fully-connected output. The input $x$ is a batch of spin configurations (shape [batch, 1, L, L], treated as 1-channel images). After two conv layers and a global pooling, we have 32 features which we feed to a linear layer for binary classification. The training loop uses cross-entropy loss to train the model to predict the correct phase label. In practice, one would train over many epochs until the network achieves high accuracy on distinguishing phases. After training, one could input new configurations at various $T$ and see the predicted probability of being in the ordered phase – the cross-over of this probability around 0.5 would indicate the estimated $T_c$.





Generative Learning: VAE for Latent Representations





Autoencoders: Another unsupervised approach uses autoencoders, neural networks trained to compress and then reconstruct data. A Variational Autoencoder (VAE) is a probabilistic generative model that learns a latent variable description of the data. It consists of an encoder network $E_\phi$ that maps an input (spin configuration) to a set of latent variables (mean & variance for each latent dimension), and a decoder network $D_\theta$ that maps a sample from this latent distribution back to the data space, attempting to reconstruct the original input . The VAE is trained by maximizing a lower bound to the data likelihood, which includes a reconstruction error term and a regularization term pushing the latent distribution toward a prior (usually an isotropic Gaussian).
VAEs for Phase Transitions: The idea is that the VAE’s latent space will capture the essential features of the configurations. If the model is well-trained, configurations from different phases might occupy different regions in latent space. Indeed, studies have found that a VAE with a 1- or 2-dimensional latent space can learn to encode spin configurations in such a way that the latent variable correlates with the order parameter (magnetization) . For example, a single latent dimension might be mapped to the magnetization of the configuration. As a result, the latent representation $z$ effectively classifies the phase: $z$ near +1 might correspond to mostly spin-up, $z$ near –1 to spin-down (ordered phases), and $z \approx 0$ to disordered configurations. This means the VAE autonomously discovers the phase structure: the latent variables cluster configurations by phase without being told about phases . This was demonstrated by Wetzel (2017), who showed that PCA and VAE both yield latent parameters corresponding to known order parameters, and that latent encodings form distinct clusters for different phases .
Detecting Criticality: How do we detect the phase transition using a VAE? One way is to look at the distribution of latent variables as a function of temperature. In the ordered phase, the latent encodings might split into two separated modes (for up vs down magnetization domains), whereas near $T_c$ the encodings spread out or the distribution changes character, and in the disordered phase they cluster around a single mode (zero magnetization) . Another way is to use the VAE’s reconstruction loss: it has been observed that the reconstruction error often peaks or changes behavior at the phase transition . Intuitively, at criticality the configurations are most “surprising” or hardest to compress, which can lead to a bump in reconstruction error – making it a possible unsupervised indicator of $T_c$. In summary, VAEs provide both a dimensionality reduction and a generative model: they not only identify phases (via latent clustering) but can also generate new configurations by sampling latent variables and decoding. Generated samples from a trained VAE qualitatively resemble the real ones and carry features of the learned distribution . For instance, decoding random latent vectors yields spin configurations whose energy–magnetization distribution looks similar to the training data’s distribution across phases (though one must be cautious: small latent spaces may fail to capture all correlations, producing unphysical samples at times ).
Anomaly Detection: Another interesting use of VAEs is treating the VAE as an anomaly detector for criticality. If you train the VAE on data mostly away from $T_c$, the critical configurations might reconstruct poorly (higher error) because they don’t fit well into either phase’s learned features. By scanning temperature, a spike in VAE reconstruction error or a significant shift in latent space usage can signal a transition . This approach doesn’t require prior labeling of phases, thus serving as a fully unsupervised phase transition detector.
Summary: VAEs, like PCA, perform unsupervised feature extraction, but with non-linear deep learning power. They bridge a gap: more expressive than linear PCA, and they provide generative capability (sampling and interpolation between configurations). Their latent dimensions can be interpreted (e.g. as order parameters) but the mapping is learned, not predefined. This makes them a promising tool for discovering unknown order parameters or subtle phase transitions in many-body systems .






Example:

 VAE Architecture & Training (PyTorch)





Figure: Basic architecture of a Variational Autoencoder. The encoder network (green) compresses an input configuration $x$ into a latent space (red) by outputting parameters $\mu(z|x)$ and $\sigma^2(z|x)$ of a probability distribution (often Gaussian). A latent vector $z$ is sampled from this distribution (illustrated by the Gaussian bell curve). The decoder network (blue) then takes $z$ and tries to reconstruct the original configuration, outputting $x’$ . Training optimizes the reconstruction accuracy while keeping the latent distribution close to a chosen prior (usually $N(0,1)$) via a Kullback-Leibler (KL) divergence term. This ensures the latent space is well-behaved and can be sampled. In formulas, the loss function for each input $x$ is $L(x) = \mathbb{E}{z\sim q\phi(z|x)}[-\ln p_\theta(x|z)] + \mathrm{KL}[,q_\phi(z|x),||,p(z),]$, combining the reconstruction log-loss and the KL regularizer . Through training, the VAE learns a compressed representation where similar configurations map to nearby points in latent space . In the context of the Ising model, “similar” often means same phase or similar magnetization, so the VAE naturally tends to organize the latent space by phases. New configurations can be generated by sampling $z$ from the prior and decoding, making the VAE a generative model for spin configurations as well .


import torch.nn as nn
import torch.nn.functional as F

class IsingVAE(nn.Module):
    def __init__(self, latent_dim=2, N_spins=100):  # e.g., 10x10 Ising has N_spins=100
        super(IsingVAE, self).__init__()
        # Encoder layers
        self.fc1 = nn.Linear(N_spins, 128)
        self.fc_mu = nn.Linear(128, latent_dim)        # outputs μ vector
        self.fc_logvar = nn.Linear(128, latent_dim)    # outputs log σ^2 vector
        # Decoder layers
        self.fc_dec1 = nn.Linear(latent_dim, 128)
        self.fc_out = nn.Linear(128, N_spins)
    def encode(self, x):
        h = F.relu(self.fc1(x))
        return self.fc_mu(h), self.fc_logvar(h)
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std             # sample z
    def decode(self, z):
        h = F.relu(self.fc_dec1(z))
        return torch.sigmoid(self.fc_out(h))  # sigmoid for [0,1] output
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_recon = self.decode(z)
        return x_recon, mu, logvar

# Training step (for one batch)
vae = IsingVAE(latent_dim=1, N_spins=100)
x_batch = batch_X.view(-1, 100)              # flatten spins
x_recon, mu, logvar = vae(x_batch)
# Compute reconstruction loss (binary cross-entropy) and KL divergence
recon_loss = F.binary_cross_entropy(x_recon, x_batch, reduction='sum')
kl_loss = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())
loss = recon_loss + kl_loss
loss.backward(); optimizer.step()
Code: The above defines a simple VAE for an Ising model with $N_{\text{spins}}$ inputs (e.g. a 10×10 lattice). The encoder is a feed-forward network that produces a length-latent_dim mean and log-variance. reparameterize samples a latent $z$. The decoder reconstructs the spin configuration (here using a sigmoid output to produce probabilities between 0 and 1 for each spin being “up”; one could also output logits). The training involves a reconstruction term (we use binary cross-entropy treating spins as 0/1) and a KL term. Over many epochs, the VAE will learn to accurately reconstruct configurations while structuring the latent space. After training, one can examine mu for each input: if latent_dim=1, plotting $\mu$ vs temperature will show a sharp change around $T_c$. If latent_dim=2, plotting the latent encodings clusters should reveal distinct phase clusters. The VAE can also generate synthetic configurations by sampling $z \sim N(0,1)$ and feeding it to decode().





Comparison of Methods: PCA vs CNN vs VAE





Dimensionality & Data: PCA and VAE are unsupervised – they do not need phase labels and can potentially discover phases on their own. CNN (in the straightforward approach) is supervised – it requires labeled training data (knowing which phase each sample is in). In terms of data quantity, CNNs typically need a large labeled dataset but reward you with high accuracy classification . PCA and VAE can work with unlabeled data; PCA is not data-hungry (it will find principal components even with modest data), whereas VAE (being a neural network) may need substantial data and tuning to learn a good representation.
Performance: If one’s goal is to locate the critical point or identify phases, all three approaches have succeeded in the Ising model example. The CNN can achieve essentially perfect classification of phases and pinpoint $T_c$ within the resolution of the temperature sampling . PCA can indicate $T_c$ by variance analysis or clustering, though determining the exact critical point might be less precise without further analysis (it gives a qualitative picture, albeit one can estimate the peak in variance or similar). VAE can also detect the transition via latent clustering or anomaly peaks ; its performance depends on the network capacity and training. In more complex scenarios (e.g. detecting a Kosterlitz-Thouless transition or a topological phase), CNNs (especially if combined with clever training schemes) have proven capable , whereas PCA might fail if no single linear feature distinguishes phases. VAEs, with non-linearity, have shown promise in capturing such transitions as well (and even generating new configurations from each phase).
Interpretability: PCA is highly interpretable – the principal components can often be understood in terms of physical observables (magnetization, etc.) . It’s essentially a transparent linear model. CNN is more of a black box with many parameters; it automatically learns features and we don’t always know what those are. For the Ising case, we later deciphered that it learned magnetization , but generally CNNs can be difficult to interpret especially as architectures become deeper . There is a whole subfield of explainable AI dealing with interpreting neural networks. VAE lies in between: the latent variables provide a relatively low-dimensional representation that we can try to interpret (e.g. correlating latent values with known physical quantities). In successful cases, one latent might correspond to the order parameter . However, VAEs still involve neural networks, and if the latent space is higher-dimensional, interpretation can be tricky (one might need to examine combinations of latent features). Overall, PCA wins in simplicity and clarity, CNN often wins in pure predictive power, and VAE offers a balance by giving a latent space that can be partly understood and a generative aspect.
Generative Capability: PCA is not generative (though one could generate data by sampling principal components with normal distributions, this typically doesn’t capture the true data distribution beyond first two moments). CNN in classification form is not generative either (it’s discriminative). The VAE is explicitly generative – after training, one can create new spin configurations by sampling from the latent space. This is a big advantage if one wants to extrapolate or infer new samples (for instance, generating hypothesized configurations at criticality to see if they look like percolating clusters, etc.). It also allows exploring the latent space: by interpolating in latent space one can morph an ordered configuration into a disordered one and observe how microscopic structure changes, offering insight into the continuum of configurations between phases.
Robustness and Limitations: PCA is a linear method – it might fail if the phase separation in data is non-linear (e.g. if configurations from two phases are intertwined in a complex manifold, a linear cut won’t separate them well). CNNs can learn very complex non-linear boundaries and thus handle practically any distinguishing pattern given enough data; however, they might also learn spurious features if not careful (e.g. finite-size effects or specific domain shapes that are not fundamental to the phase). VAEs sometimes face difficulties in training (e.g. issues like posterior collapse, or choosing the right size of latent space). Also, VAEs may “smooth out” sharp distinctions because of their probabilistic nature – e.g. mixing two phases in latent space if not clearly separated. It has been observed that for very low-dimensional latent spaces, VAEs may struggle to perfectly reconstruct configurations, especially in ordered phases (yielding unphysical artifacts like overly random spins) . Increasing latent dimensionality improves reconstruction but risks encoding trivial solutions (like remembering each configuration). Thus, one must find a sweet spot.
Use Cases: In practice, one might start with PCA to quickly check if the data has obvious two-cluster structure (indicating a phase transition). Then use a CNN for a reliable automated classification and precise critical point estimation (especially if a labeled training set can be prepared). Finally, use a VAE or other generative model to gain deeper insight into the data manifold: this can reveal if the machine is picking up an order parameter or some complex combination, and generate new samples to test hypotheses. All these methods serve as part of a modern toolkit for computational physics, where machine learning augments traditional analysis to identify and characterize phase transitions .






Conclusions & Further Directions





Summary: We have seen that machine learning (ML) provides powerful methods to classify and understand phase transitions in physical systems. The 2D Ising model served as a case study: using Monte Carlo-generated spin configurations, we applied PCA (unsupervised linear reduction), CNN (supervised deep classifier), and VAE (unsupervised deep generative model) to distinguish phases and locate the critical point. Each approach identified the phase transition in its own way – from PCA’s variance spike and clustering, to the CNN’s sharp classification accuracy drop at $T_c$, to the VAE’s latent space organization and reconstruction anomaly at criticality.
Interpretation: An encouraging theme is that ML algorithms often rediscover known physics: the principal component aligned with magnetization, the CNN effectively measured the magnetization (the order parameter) , and the VAE’s latent variable aligned with the magnetization as well . This gives confidence that ML can correctly capture physical essence. In more complex scenarios, ML might uncover order parameters not obvious to the human investigator, highlighting its potential for new discoveries.
Advantages & Trade-offs: PCA is quick and interpretable but limited to linear features. CNNs leverage data to capture complex order (even non-local order in topological phases) and are very accurate – they can detect transitions “regardless of the type of transition” given training data – but they act as black boxes and require labeled examples (which in some cases might mean relying on known information to train). VAEs and other autoencoders can operate without labels and yield a human-analyzable latent space; they also allow generation of hypothetical new states, which can be a form of simulation itself. However, they require more effort to train/tune and interpret.
Outlook: The intersection of ML and statistical physics is rapidly growing. Beyond PCA/CNN/VAE, researchers are exploring graph neural networks (for complex lattices or molecular phases), normalizing flows and other advanced generative models for more accurate sampling of configurations, and reinforcement learning to navigate phase diagrams. There are also efforts to integrate physical knowledge into ML models (through symmetry constraints or conservation laws) to improve learning efficiency. For graduate students, mastering these tools opens up new ways to tackle long-standing problems in many-body physics, materials science, and beyond. The ultimate promise is that machine learning can serve as a “mathematical microscope” to detect subtle phase transitions, or even a predictive framework to suggest where novel phases of matter might lie, thus complementing theoretical analysis and experiments in the quest to map out the rich landscape of phase transitions in nature .




Sources: This presentation is based on insights from recent literature at the interface of machine learning and physics, including applications of PCA and autoencoders to spin models , supervised neural network identification of phases , and variational autoencoder analyses of Ising model data . These demonstrate the effectiveness of ML in recognizing order parameters and critical points from raw data, as well as the importance of interpretability in bridging ML results with physical understanding .



Machine Learning for Phase Transitions in Physical Systems







Introduction to Phase Transitions and Models





Phase Transitions: A phase transition is a change in the macroscopic state of a system (e.g. solid–liquid, magnetic order–disorder) characterized by abrupt changes in physical properties. In continuous (second-order) transitions, an order parameter changes continuously (but its derivative jumps) at a critical point, whereas in first-order transitions the order parameter itself jumps discontinuously . The order parameter $\phi$ is a quantity that distinguishes phases (e.g. magnetization in a magnet); it is nonzero in the ordered phase and zero in the disordered phase . At the critical temperature $T_c$ of a second-order transition, long-range correlations emerge, leading to critical fluctuations and scale-invariant patterns.



Ising Model (2D): The Ising model is a simple statistical model of ferromagnetism consisting of spins $S_i = \pm1$ arranged on a lattice (here a 2D grid) . Each spin interacts with its nearest neighbors with coupling $J$ (favoring alignment). The Hamiltonian (energy function) is:



$$H = -J \sum_{\langle i j\rangle} S_{i} S_{j},$$



where $\langle i j\rangle$ denotes neighboring pairs . In zero external field, the 2D Ising model has a well-known continuous phase transition (Onsager’s solution) at $T_c/J = \frac{2}{\ln(1+\sqrt{2})} \approx 2.269$ . For $T < T_c$ (low temperature), spins are mostly aligned (ferromagnetic order, nonzero magnetization); for $T > T_c$ (high temperature), spins are disordered with zero net magnetization (paramagnetic phase) . At $T=T_c$, the system is critical, exhibiting fractal spin domains (no characteristic size) and diverging correlation length.



Illustration of a 2D Ising model on a square lattice. Each site has a spin up (pink) or down (blue), interacting with nearest neighbors (gray bonds). In the ordered phase at low $T$, most spins align, whereas at high $T$ they are random. The Ising model undergoes a second-order phase transition at the critical temperature $T_c$ .



Potts Model: The $q$-state Potts model generalizes Ising (which is $q=2$) to spins that can take $q$ different values. Spins on a lattice interact such that they prefer to be in the same state as their neighbors (with some coupling energy if they match) . The Potts model also exhibits phase transitions that can be of first or second order depending on $q$ and dimension (e.g. in 2D, $q\le4$ yields a continuous transition, while larger $q$ gives a first-order transition). Studying Ising and Potts models provides insight into ferromagnets and other systems . In all these models, identifying the critical point $T_c$ and the behavior of the order parameter near $T_c$ is a central task in statistical physics .



Why ML for Phase Transitions? Traditional analysis of phase transitions relies on known order parameters or response functions (magnetization, susceptibility, etc.) which may not be obvious for complex or novel systems. Machine learning offers new ways to learn order parameters and detect phase changes directly from raw state configurations, without prior assumptions . We will explore three approaches – unsupervised, supervised, and generative ML – to classify phases in Ising/Potts models, using Monte Carlo simulation data as input.





Data Generation via Monte Carlo Simulation





To apply machine learning, we first need data: spin configurations at various temperatures (both below and above $T_c$). We use Monte Carlo (MC) simulations to generate equilibrium spin states for the Ising/Potts models. A common choice is the Metropolis-Hastings algorithm (importance-sampling Markov Chain Monte Carlo) in the canonical ensemble :



Initialize the lattice with spins (random or all +1/-1).
Propose a random spin flip at some site.
Compute the energy change $\Delta E$ from flipping that spin.
Accept or reject the flip: if $\Delta E < 0$ (energy lowers) accept unconditionally; if $\Delta E > 0$, accept with probability $\exp(-\Delta E / k_B T)$ . This satisfies detailed balance at temperature $T$.
Repeat many times (sweeps) to reach equilibrium and sample uncorrelated configurations.




This Metropolis MC generates samples according to the Boltzmann distribution $P({\sigma}) \propto \exp(-H/k_BT)$. We simulate across a range of temperatures (e.g. from well below $T_c$ to well above $T_c$) to obtain labeled datasets of spin configurations in the ordered and disordered phases. For example, in a 2D Ising model of size $L\times L$, one might collect thousands of spin configurations at each temperature $T$ (after sufficient equilibration sweeps) to serve as training data.



Data characteristics: Below $T_c$, configurations have large magnetized domains (most spins up or down); above $T_c$, configurations are noisy with roughly equal up/down spins . Exactly at $T_c$, critical fluctuations yield characteristic clusters of all sizes. These differences (order vs disorder) are what we hope a machine learning algorithm can learn to recognize. However, near the critical point, configurations are very mixed and stochastic, making classification challenging. Thus, a good ML model may help pin down the precise $T_c$ by identifying when the system’s “pattern” changes.



(Note: For efficiency, advanced MC methods like cluster algorithms (Wolff or Swendsen-Wang) can be used to reduce critical slowing down, but the basic data for ML is the same: a large collection of spin state matrices labeled by the simulation temperature or phase.)





Unsupervised Learning: PCA for Phase Detection





Principle Component Analysis (PCA): PCA is a linear dimensionality reduction technique that finds orthogonal directions (principal components) of maximum variance in the data . By projecting high-dimensional data (here, spin configurations) onto the first few principal components, we capture the most significant variations. The idea is that different phases might occupy different regions in this reduced feature space, enabling us to detect phase transitions without knowing labels in advance . Importantly, PCA makes no prior assumption about an order parameter or critical point – it simply finds directions in spin configuration space that have the largest variance across all samples.



Applying PCA to Ising Model: We represent each spin configuration as a vector $x \in \mathbb{R}^{N}$ (with $N=L^2$ spins flattened into a vector of +1/−1). Running PCA on a set of configurations across various $T$, we obtain principal components $y_\ell = w_\ell \cdot x$ (linear combinations of spins) ranked by how much variance they explain . For the 2D Ising model, it turns out that the first principal component corresponds to the overall magnetization of the configuration – effectively the sum of spins . This makes sense: the largest variation in the data is between mostly-up-spin states at low $T$ and mixed states at high $T$ (with magnetization ~0). Thus, PCA automatically discovers the magnetization as the key distinguishing feature (order parameter) for the Ising transition .



PCA projection of Ising model spin configurations (2D $20\times20$ lattice) for two temperature regimes: $T \ll T_c$ (ordered, red points) and $T \gg T_c$ (disordered, blue points). We flatten each configuration into a 400-dimensional vector and project onto the first two principal components. PC1 clearly separates the two phases by capturing the magnetization (ordered states have a large negative PC1, disordered states have positive PC1 in this example). PC2 carries much less variance. This unsupervised separation implies PCA identifies the phase transition feature without knowing any labels.



By examining the PCA outputs, one can estimate the critical point. For example, as temperature increases, the distribution of samples along PC1 will shift from one cluster (ordered) to another (disordered). The point where the variance along PC1 (or the cluster separation) is maximal or changes rapidly corresponds to $T_c$. Indeed, studies have shown that quantitative PCA can locate critical points and distinguish different types of transitions . In one study, the principal component clearly indicated the symmetry-breaking phase transition and even differentiated first vs second order transitions in various spin models . By looking at the explained variance ratio, one often sees a peak or sudden change at $T_c$ . Thus, unsupervised PCA provides a quick way to detect a phase transition from raw data.



However, PCA has limitations: it only captures linear correlations. If the order parameter is a non-linear function of the spins or if the phase transition involves more subtle patterns (e.g. topological order or complex degeneracies), PCA might not find a clear separation . For instance, a frustrated spin system might have an order parameter that is not simply the sum of spins, and PCA might fail to identify the phase unless higher principal components or non-linear methods are used . Despite this, PCA is a powerful first step and often yields interpretable results (principal components can be interpreted as approximate order parameters ). It exemplifies how emergent collective behavior can be detected by dimensionality reduction in an unsupervised way.



PCA Implementation (PyTorch): In practice, implementing PCA can be done via singular value decomposition (SVD) on the data matrix. For example, using PyTorch one could do:

import torch
# Suppose data_tensor shape = (num_samples, L, L) with spins ±1.
X = data_tensor.view(num_samples, -1).float()       # Flatten configurations
X = X - X.mean(dim=0)                               # Center the data
U, S, Vh = torch.linalg.svd(X)                      # Singular Value Decomp.
principal_components = torch.matmul(X, Vh.T[:, :2]) # Project onto first 2 PCs
explained_var = (S**2) / (S**2).sum()               # variance explained by each PC
This would yield the principal component scores (principal_components) for each sample and the explained variance for each component. One can then analyze principal_components[:,0] (PC1) as a function of temperature to find $T_c$. In our Ising example, PC1 is strongly correlated with magnetization, so it jumps from negative to positive (or vice versa) around $T_c$.





Supervised Learning: CNNs for Phase Classification





Supervised Approach: In supervised learning, we use labeled data — here the label could be the phase (e.g. “ordered” vs “disordered”) or the temperature of the sample. A model is trained to predict the label from the input spin configuration. If successful, such a model effectively learns to distinguish phases of matter. One powerful supervised model for image-like data is the Convolutional Neural Network (CNN).



Convolutional Neural Networks: CNNs are specifically designed to recognize spatial patterns. They apply learned convolution filters that scan across the lattice, picking up local features (like clusters of up or down spins, domain walls, etc.). Through multiple convolutional layers and nonlinear activations (ReLU, etc.), a CNN can hierarchically extract higher-level features from lower-level ones. Ultimately, fully-connected layers (dense layers) combine these features to output a prediction (e.g. a binary classification of phase).



For the Ising model, a CNN can learn to detect magnetic domains or correlation patterns. For example, in the ordered phase, it might detect large uniform regions of +1 or -1 spins, whereas in the disordered phase it sees a noisy mix. At or near $T_c$, the task is hardest because of critical fluctuations – the CNN must identify more subtle cues. Remarkably, CNNs have been shown to detect phase transitions without explicit guidance. In one study, researchers trained a deep CNN on Ising model configurations labeled by temperature and found it could automatically pinpoint the critical temperature – the CNN’s predictions or internal activations signaled the transition point, even though it was not told about phases explicitly . In fact, the CNN was able to define a new effective order parameter from its learned features that estimated $T_c$ accurately .



Cross-model generalization: An interesting finding is that a CNN trained on one model can sometimes generalize to others. For instance, a CNN trained to predict temperatures of 2D Ising configurations was able to detect the phase transition in the $q$-state Potts model with high accuracy, even though the Potts configurations look different (for $q>2$) . This suggests the CNN learned a general representation of ordering vs disorder (like recognizing clusters vs randomness). The deep CNN output was essentially acting as a continuous “temperature meter” and showed a sharp change at the critical point for any $q$, highlighting that the network found the underlying transition feature universally .



CNN Architecture Example: We can design a simple CNN for classifying Ising configurations into two classes (below $T_c$ vs above $T_c$). For example, in PyTorch:

import torch.nn as nn
import torch.nn.functional as F

class IsingPhaseCNN(nn.Module):
    def __init__(self, L):
        super(IsingPhaseCNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)
        self.fc1   = nn.Linear(16 * L * L, 2)  # output 2 classes (0=ordered, 1=disordered)
    def forward(self, x):
        x = F.relu(self.conv1(x))      # conv layer 1 + ReLU
        x = F.relu(self.conv2(x))      # conv layer 2 + ReLU
        x = x.view(x.size(0), -1)      # flatten feature maps
        x = self.fc1(x)                # linear output layer
        return x

model = IsingPhaseCNN(L=32)
Here we used two convolutional layers (with 8 and 16 filters) followed by a fully-connected layer. Even this small network can achieve high accuracy in phase classification for reasonably sized lattices. We would train it on a dataset of spin configurations with labels (e.g. 0 for $T<T_c$, 1 for $T>T_c$) using a loss like cross-entropy. In training, the CNN will adjust its filters to detect useful features. For example, one filter might respond to local magnetization (counting majority spins in its receptive field), another might detect domain walls, etc. Over epochs of training, the network improves its ability to distinguish phases.



Training and Performance: Using an optimizer like Adam, we can train the CNN until it classifies a separate test set with high accuracy. Typically, one finds that away from the critical region, the CNN reaches ~100% accuracy (ordered vs disordered states are easy to tell apart). Near $T_c$, even humans might not distinguish a single snapshot, and indeed the CNN’s confidence (or accuracy on states at $T\approx T_c$) will dip. One approach to locate $T_c$ is to see where the CNN is most confused. This is the basis of the “confusion scheme” where one trains a classifier assuming a trial critical temperature and finds the value that maximizes confusion (lowest accuracy) corresponds to the true $T_c$ . In general, a trained CNN can be used to scan through temperature and its output (or prediction probabilities) will change from one class to the other around $T_c$, thus identifying the transition.



Supervised CNNs tend to be very effective: they can learn non-linear combinations of spins, capturing complex correlations beyond the linear order parameter. They have successfully identified even more complex phases (including topological phases, when given appropriate data encoded as images) in other studies. The drawback is that CNNs require labeled training data. For physical systems, this means we need prior knowledge to label phases or at least a range where phases are known. In our case, we labeled by temperature or phase from simulations – which is feasible since we know the model. But if we had experimental data without clear labels, training a CNN would be less straightforward (we might then resort to unsupervised or generative methods).



Nonetheless, in many physics applications, simulation provides the ground truth for training, and then the CNN could potentially be applied to experimental observations to detect phases. Interpretability of CNNs is an ongoing challenge – they are often treated as black boxes. However, as mentioned, one can sometimes interpret a neuron or an output as an “order parameter” proxy or use visualization techniques to see what features the CNN is looking at. In summary, CNNs bring powerful pattern recognition to phase transition problems, achieving high performance in classification tasks and even generalizing insights across systems .





Generative Learning: Variational Autoencoders (VAEs)





Autoencoder concept: A Variational Autoencoder (VAE) is a generative model that learns to encode data into a low-dimensional latent representation and then decode from that representation back to the original data space. It consists of an encoder network $q_\phi(z|x)$ that maps an input $x$ (here a spin configuration) to a distribution over latent variables $z$, and a decoder network $p_\theta(x|z)$ that tries to reconstruct the original data from $z$. VAEs are trained by maximizing the evidence lower bound (ELBO), which can be written as:



$$\mathcal{L}(\phi,\theta) = \mathbb{E}{q\phi(z|x)}[-\ln p_\theta(x|z)] + \mathrm{KL}(q_\phi(z|x),|,p(z)),$$



where the first term is the reconstruction error and the second is a Kullback-Leibler divergence regularizer that pushes $q_\phi(z|x)$ towards a prior $p(z)$ (often a standard normal) . Intuitively, the VAE learns to compress the important information of the input into the latent variables $z$, while ensuring $z$ follows a reasonable distribution.



Using VAEs for phase transitions: Since VAEs do not require labels (unsupervised), we can feed in spin configurations at all temperatures and let the model learn a latent representation of this ensemble. The hope is that the latent space will organize configurations according to their similarity, potentially clustering configurations by phase. For example, the VAE might learn one latent dimension that corresponds to the magnetization of the configuration (similar to PCA’s first component but now possibly in a non-linear way). Indeed, studies have found that autoencoders can capture phases and even locate critical points – effectively learning the order parameter or distinguishing feature on their own . One case showed that a VAE trained on 2D Ising model data could reproduce the entire phase diagram (mapping out ordered vs disordered regions) without any explicit definition of an order parameter . This means the VAE’s latent variables encoded the phase information spontaneously.



How does a VAE indicate a phase transition? One way is to look at the distribution of latent encodings for data at different $T$. If one latent dimension corresponds to magnetization, as $T$ crosses $T_c$ the distribution of that latent variable will shift from a distribution centered near $\pm$ some value (bimodal for the two symmetry-broken magnetization directions) to a distribution centered at 0 (paramagnetic). The point of maximal variance in that latent or a sudden change in latent clustering would signal $T_c$. Another way is to look at reconstruction quality: an autoencoder might reconstruct ordered and disordered configurations well but if asked to reconstruct a configuration at $T_c$ (which might lie “between” the learned representations), it could have higher error. Some researchers have indeed used anomalies in autoencoder reconstruction error to detect transitions . Overall, a VAE can act as a probe: since it must compress the data, it will focus on the most salient features—often the presence or absence of order.



VAE example architecture: For simplicity, one can use fully-connected layers on flattened spin configurations. For instance, a small VAE in PyTorch:

class IsingVAE(nn.Module):
    def __init__(self, N=400, z_dim=2):
        super(IsingVAE, self).__init__()
        # Encoder layers
        self.fc1 = nn.Linear(N, 128)
        self.fc_mu = nn.Linear(128, z_dim)
        self.fc_logvar = nn.Linear(128, z_dim)
        # Decoder layers
        self.fc2 = nn.Linear(z_dim, 128)
        self.fc3 = nn.Linear(128, N)
    def encode(self, x):
        h = F.relu(self.fc1(x))
        return self.fc_mu(h), self.fc_logvar(h)   # outputs for mean and log-variance
    def decode(self, z):
        h = F.relu(self.fc2(z))
        x_recon = torch.tanh(self.fc3(h))         # use tanh to get outputs in [-1,1]
        return x_recon
    def forward(self, x):
        mu, logvar = self.encode(x)
        # reparameterization trick
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return self.decode(z), mu, logvar
During training, one would compute the VAE loss for each batch as:

x_recon, mu, logvar = model(x_batch)
recon_loss = F.mse_loss(x_recon, x_batch, reduction='sum')
kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
loss = recon_loss + kl_loss
Minimizing this encourages accurate reconstruction while keeping $q_\phi(z|x)$ close to a normal distribution. After training on many configurations (spanning different $T$), we can analyze the learned latent space. For example, if $z$ is 2-dimensional, we can plot the encoded $z$ values for configurations colored by temperature. We might see that low-$T$ configs cluster in one part of $z$-space and high-$T$ configs in another, with a transition around $T_c$ where the clusters move or overlap.



Insights from VAE: A VAE is a probabilistic model, so we can generate new spin configurations by sampling $z$ from the prior $p(z)$ and passing it through the decoder. In the ordered phase, sampling a certain range of $z$ might yield all spins up or all down configurations (with some noise), whereas sampling a different range yields disordered-looking patterns. This generation ability not only confirms what the VAE has learned, but also can be used to interpolate between phases. One can try values of $z$ that interpolate between an ordered and disordered latent code and decode them; somewhere in between, the decoded configurations should look critical. Such generative exploration can potentially reveal the nature of the transition (e.g. how rapidly order disappears).



Researchers have successfully used VAEs to detect phase transitions without an order parameter. One study on an anisotropic 2D Ising model demonstrated that a VAE could reproduce the phase diagram across a range of coupling parameters and temperatures, identifying the critical line in the model purely from the encoded latent space . This means the VAE learned the relationship between interaction anisotropy, temperature, and order–disorder behavior, effectively acting as a scientific discovery tool. The latent variables in that case could be mapped to a combination of energy and magnetization features the system exhibits .



However, like any neural network, a VAE’s latent features might be hard to interpret if they mix multiple physical factors. They also require more data and training time than PCA. VAEs, being generative, can suffer from issues like mode collapse or poor reconstructions if not tuned well. But when successfully trained, they offer a rich understanding: not only can they classify phases, they also model the entire data distribution, which means they capture fluctuations within each phase as well.



In summary, generative models like VAEs provide an unsupervised, physics-agnostic way to learn phase transitions, discovering order parameters and criticality by compressing and rebuilding configurations. They complement the simpler PCA by capturing non-linear features and complement CNNs by not requiring labels. The trade-off is higher complexity and sometimes less direct interpretability (though latent dimensions can often be correlated with known physical quantities post hoc).





Comparative Evaluation of Methods





Now we compare the three approaches – PCA (unsupervised linear), CNN (supervised discriminative), and VAE (unsupervised generative) – in terms of their performance, interpretability, and use cases:



Principal Component Analysis (PCA):
Performance: Fast and easy to apply. It successfully detects phase transitions in systems where the variance is dominated by the order parameter . For the Ising model, PCA clearly signals $T_c$ by the change in the leading principal component distribution. However, PCA might fail for more complex transitions if the key feature is not a single linear combination of spins (e.g. topological phases or multipolar orders).
Interpretability: Very high. The principal components can often be directly interpreted (e.g. PCA found magnetization direction for Ising ). This provides physical insight – PCA essentially finds the known order parameter (if one exists) or suggests a candidate order parameter from data. That weight vector can hint at which spins or correlations are important.
Use Cases: Great as an initial exploratory tool when you have configuration data and suspect a phase transition but don’t know the order parameter. It’s unsupervised and needs no labeling or training hyperparameters. PCA is commonly used in physics to reduce data dimensions (it has been applied to spin models, percolation, even experimental imaging of cold atoms, etc. to find transitions). It is less useful if the phase transition is very subtle or requires higher-order correlations (then one might try kernel PCA or other non-linear dimensionality reduction).

Convolutional Neural Network (CNN):
Performance: Excellent at classification tasks. A well-trained CNN can achieve ~100% accuracy in identifying phases of the Ising or Potts models (except exactly at $T_c$ where fluctuations cause some ambiguity). CNNs can also generalize – as shown by transferring an Ising-trained CNN to detect transitions in Potts models – indicating robust feature learning. They can handle large system sizes and learn complex spatial patterns (like stripy or checkerboard orders, in other models).
Interpretability: Moderate to low. While the CNN clearly works, understanding what exactly each filter or neuron represents physically is harder. There have been attempts (e.g. looking at the learned filters or saliency maps) to interpret them; in some cases, the CNN effectively learns something like magnetization or energy as intermediate quantities . One study even defined a “CNN order parameter” from the network’s output that matched the expected behavior . Overall, though, CNNs act more like black boxes compared to PCA.
Use Cases: Best when you have labeled data or a well-defined classification goal. If you know the phases present in your system (even roughly) or you want a precise detector of a known transition, CNNs are ideal. They have been used not just for thermal phase transitions but also to classify quantum phases (feeding in, say, spin configurations from quantum Monte Carlo or experimental data) and even detect phase transitions from raw images in experiments. CNNs are also useful if your order parameter is complicated (e.g. requires recognizing a pattern) – the CNN will learn that pattern. The downside is the need for a training set and computational resources (training can be slow for large networks and system sizes). CNNs also might overfit if data is limited, though techniques like dropout and regularization help.

Variational Autoencoder (VAE) / Generative Models:
Performance: Good at qualitatively identifying transitions, though the results may be less sharp than a supervised classifier. A VAE can definitely indicate the presence of a phase transition (e.g. via clustering in latent space) and has successfully mapped phase diagrams . The generative ability means it learns the entire distribution of configurations for each phase – this is a richer task than just classification. That richness sometimes makes training harder; VAEs might require tuning to get meaningful latent representations (one has to choose the latent dimension, etc.). In terms of precision, a VAE might not pinpoint $T_c$ as exactly as a CNN trained explicitly to discriminate around $T_c$, but it will come close (one can often locate the point of maximal change in latent features or reconstruction error).
Interpretability: Varies. If the latent dimension is small (2 or 3), one can visualize it and often find one axis corresponds to the order parameter. For example, one latent variable might track magnetization, while another might track some independent feature like energy or something specific to the dataset (perhaps the particular realization of domains) . Because the VAE is probabilistic, one can also examine the decoder’s output: for instance, input a latent vector and see what configuration is generated – this can tell you what the latent variables mean (e.g. setting a certain $z$ to a large value generates an ordered state). In the anisotropic Ising study, the latent variables from the VAE could be related back to known analytic results, reinforcing that they had physical meaning . However, if the latent space is high-dimensional, interpretability drops (though one could then use PCA on the latent vectors!).
Use Cases: Useful when you don’t have labels at all, or when you want not just to classify but to learn the data distribution. Autoencoders can be applied to experimental data where phases are unknown – by clustering latent encodings you might discover new phases. They are also valuable for anomaly detection (a phase transition can be treated as an anomaly between two learned phases). VAEs (and other generative models like GANs) have been used in detecting transitions in fields like astrophysics, structural biology, etc., where one wants to let the model tell us if there’s a change in pattern. Additionally, generative models allow data augmentation – you can generate synthetic samples for further analysis. The cost is that training is more complex (two networks plus careful tuning to avoid issues like blurry reconstructions).





In summary, PCA is simple and interpretable but limited, CNNs are powerful and precise but require prior knowledge (labels) and are less interpretable, and VAEs are very flexible and can discover new physics unsupervised but are complex to train and interpret. These methods are not exclusive — they can complement each other. For instance, one could use PCA to get a quick estimate of $T_c$, use that to label data, train a CNN for high-accuracy classification, and use a VAE to further explore the structure of configurations and generate new ones. In the modern exploration of phase transitions, all three approaches have proven valuable.





Conclusion and Outlook





We have seen that machine learning provides a diverse toolkit for tackling the problem of identifying and classifying phase transitions in physical systems:



Theoretical recap: Using the Ising and Potts models as examples, we understand phase transitions in terms of symmetry breaking and order parameters (magnetization for Ising, etc.). These models provide a playground for ML since we can generate many samples via Monte Carlo and we know their phase diagrams.
Unsupervised PCA: a quick way to reduce data that reveals the order parameter and critical point from raw configurations . It required no prior labeling and showed how variance in spin configurations is linked to physical phases.
Supervised CNN: a powerful classifier that learned to distinguish phases with high accuracy, even finding its own representation of the order parameter . With labeled data, it can generalize and detect transitions in related systems . This demonstrates how pattern recognition can automate phase identification tasks that might be tedious by traditional means.
Generative VAE: an advanced approach that learned the underlying distribution of configurations, encoding them in a latent space where phases become separated . The VAE could indicate the transition and generate new samples, pointing toward a deep understanding of the data manifold for each phase.




Each approach has strengths and weaknesses, but together they show the potential of ML in physics. Importantly, these methods don’t replace traditional analysis but rather augment it: an ML algorithm might flag a possible phase transition or suggest an order parameter, which physicists can then confirm and study using theory or experiments. As a pedagogical tool, these examples also help students intuitively grasp concepts like order parameters (since the ML finds them) and criticality (since the ML’s performance often changes at $T_c$).



Outlook: The application of machine learning to physical phase transitions is a rapidly developing field. Beyond PCA/CNN/VAE, researchers are exploring clustering algorithms, graph neural networks for complex lattices, reinforcement learning to discover new phases, and more. Unsupervised deep learning methods (like clustering in the latent space of an autoencoder, or newer techniques such as normalizing flows) are being used to detect topological phases or phase transitions in quantum systems where no local order parameter exists. There is also interest in interpretable ML, where the goal is not just to have a black-box classifier, but to extract human-understandable insight (for example, using symbolic regression on neural network outputs to see if it matches known physical formulae).



Another promising direction is transfer learning – as we saw with the CNN and Potts model, an ML model trained on one system can sometimes be applied to another. This hints at a form of universal representation of order/disorder that ML can capture. In the future, one could imagine training a general “phase transition detector” network on many models, and then applying it to experimental data (like images of spins or other degrees of freedom) to identify unknown phase transitions in real materials.



Finally, generative models might be used not just to learn phases but to propose new states of matter. For instance, a VAE or GAN trained on known phases could interpolate to suggest configurations that correspond to novel intermediate phases, which could then be tested in simulations or experiments.



In conclusion, machine learning is becoming an indispensable tool in the study of phase transitions, offering new ways to classify, interpret, and even discover phases of matter. By combining theoretical knowledge (Ising/Potts models, order parameters, critical phenomena) with data-driven techniques (PCA, CNNs, VAEs), we gain a deeper and more automated understanding of when and how matter changes its state . This multidisciplinary approach is empowering physicists to tackle long-standing problems and explore uncharted territories in phase transition research. The lecture presented a snapshot through examples; graduate students equipped with these tools can apply them to their own research problems, potentially uncovering new physics with the help of machine learning.

