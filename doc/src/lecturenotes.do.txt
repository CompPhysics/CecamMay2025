TITLE: Using Machine Learning to Classify Phase Transitions
AUTHOR: Morten Hjorth-Jensen at Department of Physics and Center for Computing in Science Education, University of Oslo, Norway
DATE: CEACM Flagship School: Machine Learning in Physical Sciences: Theory and Applications, May 26-30, 2025


!split
===== What is this about? =====
!bblock
These notes, with pertinent exercises cover the following topics. 
o Phase Transitions & Critical Phenomena: Definitions and key concepts (order parameters, critical points, first vs second order).
o Spin Models: 2D Ising model and the q-state Potts model (examples of phase transitions).
o Data Generation: Monte Carlo simulations for sampling spin configurations across temperatures.
o Unsupervised Learning (PCA): Principal Component Analysis to visualize phase separation without labels.
o Supervised Learning (CNN): Convolutional Neural Networks for classifying phases from raw configurations.
o Generative Models (VAE): Variational Autoencoders for latent representation learning and critical anomaly detection.
o Comparisons: Interpretability and performance trade-offs between PCA, CNN, and VAE.
!eblock

!split
===== Where do I find the material? =====

All the material here can be found in the PDF files, codes and jupyter-notebooks at the above _doc_ folder, see the _pub_ subfolder, see URL:"https://github.com/CompPhysics/CecamMay2025"


!split
=====  AI/ML and some statements you may have heard (and what do they mean?)  =====

o Fei-Fei Li on ImageNet: _map out the entire world of objects_ ("The data that transformed AI research":"https://cacm.acm.org/news/219702-the-data-that-transformed-ai-research-and-possibly-the-world/fulltext")
o Russell and Norvig in their popular textbook: _relevant to any intellectual task; it is truly a universal field_ ("Artificial Intelligence, A modern approach":"http://aima.cs.berkeley.edu/")
o Woody Bledsoe puts it more bluntly: _in the long run, AI is the only science_ (quoted in Pamilla McCorduck, "Machines who think":"https://www.pamelamccorduck.com/machines-who-think")


If you wish to have a critical read on AI/ML from a societal point of view, see "Kate Crawford's recent text Atlas of AI":"https://www.katecrawford.net/". 

_Here: with AI/ML we intend a collection of machine learning methods with an emphasis on statistical learning and data analysis_



!split
===== Types of machine learning =====

!bblock
The approaches to machine learning are many, but are often split into two main categories. 
In *supervised learning* we know the answer to a problem,
and let the computer deduce the logic behind it. On the other hand, *unsupervised learning*
is a method for finding patterns and relationship in data sets without any prior knowledge of the system.

An important  third category is  *reinforcement learning*. This is a paradigm 
of learning inspired by behavioural psychology, where learning is achieved by trial-and-error, 
solely from rewards and punishment.
!eblock

!split
===== Main categories =====
!bblock
Another way to categorize machine learning tasks is to consider the desired output of a system.
Some of the most common tasks are:

  * Classification: Outputs are divided into two or more classes. The goal is to   produce a model that assigns inputs into one of these classes. An example is to identify  digits based on pictures of hand-written ones. Classification is typically supervised learning.

  * Regression: Finding a functional relationship between an input data set and a reference data set.   The goal is to construct a function that maps input data to continuous output values.

  * Clustering: Data are divided into groups with certain common traits, without knowing the different groups beforehand.  It is thus a form of unsupervised learning.
!eblock



!split
=====  The plethora  of machine learning algorithms/methods =====

o Deep learning: Neural Networks (NN), Convolutional NN, Recurrent NN, Boltzmann machines, autoencoders and variational autoencoders  and generative adversarial networks, stable diffusion and many more generative models
o Bayesian statistics and Bayesian Machine Learning, Bayesian experimental design, Bayesian Regression models, Bayesian neural networks, Gaussian processes and much more
o Dimensionality reduction (Principal component analysis), Clustering Methods and more
o Ensemble Methods, Random forests, bagging and voting methods, gradient boosting approaches 
o Linear and logistic regression, Kernel methods, support vector machines and more
o Reinforcement Learning; Transfer Learning and more 



!split
===== Example of generative modeling, "taken from Generative Deep Learning by David Foster":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"  =====

FIGURE: [figures/generativelearning.png, width=900 frac=1.0]


!split
===== Example of discriminative modeling, "taken from Generative Deeep Learning by David Foster":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"  =====


FIGURE: [figures/standarddeeplearning.png, width=900 frac=1.0]



!split
===== Taxonomy of generative deep learning, "taken from Generative Deep Learning by David Foster":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"  =====

FIGURE: [figures/generativemodels.png, width=900 frac=1.0]


!split
===== Good books with hands-on material and codes =====
!bblock
* "Sebastian Rashcka et al, Machine learning with Sickit-Learn and PyTorch":"https://sebastianraschka.com/blog/2022/ml-pytorch-book.html"
* "David Foster, Generative Deep Learning with TensorFlow":"https://www.oreilly.com/library/view/generative-deep-learning/9781098134174/ch01.html"
* "Bali and Gavras, Generative AI with Python and TensorFlow 2":"https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2"
!eblock

All three books have GitHub addresses from where  one can download all codes. We will borrow most of the material from these three texts as well as 
from Goodfellow, Bengio and Courville's text "Deep Learning":"https://www.deeplearningbook.org/"




!split
===== What are the basic Machine Learning ingredients? =====
!bblock
Almost every problem in ML and data science starts with the same ingredients:
* The dataset $\bm{x}$ (could be some observable quantity of the system we are studying)
* A model which is a function of a set of parameters $\bm{\alpha}$ that relates to the dataset, say a likelihood  function $p(\bm{x}\vert \bm{\alpha})$ or just a simple model $f(\bm{\alpha})$
* A so-called _loss/cost/risk_ function $\mathcal{C} (\bm{x}, f(\bm{\alpha}))$ which allows us to decide how well our model represents the dataset. 

We seek to minimize the function $\mathcal{C} (\bm{x}, f(\bm{\alpha}))$ by finding the parameter values which minimize $\mathcal{C}$. This leads to  various minimization algorithms. It may surprise many, but at the heart of all machine learning algortihms there is an optimization problem. 
!eblock

!split
===== Low-level machine learning, the family of ordinary least squares methods  =====

Our data which we want to apply a machine learning method on, consist
of a set of inputs $\bm{x}^T=[x_0,x_1,x_2,\dots,x_{n-1}]$ and the
outputs we want to model $\bm{y}^T=[y_0,y_1,y_2,\dots,y_{n-1}]$.
We assume  that the output data can be represented (for a regression case) by a continuous function $f$
through
!bt
\[
\bm{y}=f(\bm{x})+\bm{\epsilon}.
\]
!et

!split
===== Setting up the equations =====

In linear regression we approximate the unknown function with another
continuous function $\tilde{\bm{y}}(\bm{x})$ which depends linearly on
some unknown parameters
$\bm{\theta}^T=[\theta_0,\theta_1,\theta_2,\dots,\theta_{p-1}]$.

The input data can be organized in terms of a so-called design matrix 
with an approximating function $\bm{\tilde{y}}$ 
!bt
\[
\bm{\tilde{y}}= \bm{X}\bm{\theta},
\]
!et


!split
===== The objective/cost/loss function =====

The  simplest approach is the mean squared error
!bt
\[
C(\bm{\Theta})=\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\left\{\left(\bm{y}-\bm{\tilde{y}}\right)^T\left(\bm{y}-\bm{\tilde{y}}\right)\right\},
\]
!et
or using the matrix $\bm{X}$ and in a more compact matrix-vector notation as
!bt
\[
C(\bm{\Theta})=\frac{1}{n}\left\{\left(\bm{y}-\bm{X}\bm{\theta}\right)^T\left(\bm{y}-\bm{X}\bm{\theta}\right)\right\}.
\]
!et
This function represents one of many possible ways to define the so-called cost function.


!split
===== Training solution  =====

Optimizing with respect to the unknown parameters $\theta_j$ we get 
!bt
\[
\bm{X}^T\bm{y} = \bm{X}^T\bm{X}\bm{\theta},  
\]
!et
and if the matrix $\bm{X}^T\bm{X}$ is invertible we have the optimal values
!bt
\[
\hat{\bm{\theta}} =\left(\bm{X}^T\bm{X}\right)^{-1}\bm{X}^T\bm{y}.
\]
!et

We say we 'learn' the unknown parameters $\bm{\theta}$ from the last equation.




!split
===== Selected references =====
!bblock
* "Mehta et al.":"https://arxiv.org/abs/1803.08823" and "Physics Reports (2019)":"https://www.sciencedirect.com/science/article/pii/S0370157319300766?via%3Dihub".
* "Machine Learning and the Physical Sciences by Carleo et al":"https://link.aps.org/doi/10.1103/RevModPhys.91.045002"
* "Artificial Intelligence and Machine Learning in Nuclear Physics, Amber Boehnlein et al., Reviews Modern of Physics 94, 031003 (2022)":"https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.94.031003" 
* "Dilute neutron star matter from neural-network quantum states by Fore et al, Physical Review Research 5, 033062 (2023)":"https://journals.aps.org/prresearch/pdf/10.1103/PhysRevResearch.5.033062"
* "Neural-network quantum states for ultra-cold Fermi gases, Jane Kim et al, Nature Physics Communcication, submitted":"https://doi.org/10.48550/arXiv.2305.08831"
* "Message-Passing Neural Quantum States for the Homogeneous Electron Gas, Gabriel Pescia, Jane Kim et al. arXiv.2305.07240,":"https://doi.org/10.48550/arXiv.2305.07240"
* "Efficient solutions of fermionic systems using artificial neural networks, Nordhagen et al, Frontiers in Physics
!eblock





!split
===== Machine learning. A simple perspective on the interface between ML and Physics =====

FIGURE: [figures/mlimage.png, width=800 frac=1.0]


!split
===== ML in Nuclear  Physics (or any field in physics) =====

FIGURE: [figures/ML-NP.pdf, width=900 frac=1.0]




!split
===== Phase Transitions and Critical Phenomena =====

o Definition: A phase transition is characterized by an abrupt, non-analytic change in a macroscopic property of a system as some external parameter (e.g. temperature) is varied . In simpler terms, the system’s state or phase changes dramatically at a critical point.
o Order Parameter: Associated with each phase transition is an order parameter – a quantity that is zero in one phase and non-zero in the other. For example, magnetization plays the role of an order parameter in magnetic systems, distinguishing ordered (magnetized) from disordered (unmagnetized) phases.
o Critical Point: At the critical temperature (or pressure, etc.), the order parameter changes (continuous or discontinuous) and the system exhibits critical phenomena: large fluctuations, divergence of correlation length, and the onset of scale invariance. Critical points of second-order transitions feature continuous change of the order parameter with characteristic critical exponents and universal behavior across different systems.




!split
===== Phase Transitions: Definitions =====
o A *phase transition* is a qualitative change in the state of a system when a control parameter (e.g.\ temperature) passes a critical point.
o _Order parameter_: quantity that distinguishes phases (e.g.\ magnetization $M$ for magnetic systems).
o _Order vs disorder_: e.g.\ below $T_c$ a ferromagnet has $|M|>0$ (ordered), above $T_c$ $M=0$ (disordered).
o Phases can break symmetries; transitions can be \emph{continuous} (second-order) or \emph{first-order}.

!split
===== Order Parameter and Symmetry Breaking =====
o Phase transitions often involve spontaneous symmetry breaking (e.g.\ Ising model $Z_2$ symmetry).
o The order parameter (e.g.\ magnetization $M = \frac{1}{N}\sum_i s_i$) changes behavior at $T_c$.
o In ferromagnets: $M=0$ for $T>T_c$ (symmetric paramagnet), $M\neq 0$ for $T<T_c$ (broken symmetry).
o Example: in 2D Ising model, two symmetric ordered states (up/down) below $T_c$.


!split
===== Phase Transitions: Overview =====


o _Definition_: A phase transition is characterized by an abrupt, non-analytic change in a macroscopic property of a system as some external parameter (e.g. temperature) is varied . In simpler terms, the system’s state or phase changes dramatically at a critical point.
o _Order Parameter_: Associated with each phase transition is an order parameter – a quantity that is zero in one phase and non-zero in the other. For example, magnetization plays the role of an order parameter in magnetic systems, distinguishing ordered (magnetized) from disordered (unmagnetized) phases.
o _Critical Point_: At the critical temperature (or pressure, etc.), the order parameter changes (continuous or discontinuous) and the system exhibits critical phenomena: large fluctuations, divergence of correlation length, and the onset of scale invariance. Critical points of second-order transitions feature continuous change of the order parameter with characteristic critical exponents and universal behavior across different systems.





!split
===== Phase Transitions: First vs Second Order =====


o _First-Order vs Second-Order_: In a first-order transition, the order parameter changes discontinuously at the transition (often with latent heat), whereas in a second-order (continuous) transition, the order parameter goes to zero continuously at $T_c$, accompanied by diverging susceptibility and correlation length. For example, the liquid–gas transition (at sub-critical pressures) is first-order, while the ferromagnetic transition in the 2D Ising model is second-order (continuous).
o _Example, Potts Model Transitions_: The q-state Potts model generalizes Ising (which is q=2). In 2D, the Potts model undergoes a continuous transition for $q \le  4$ and a discontinuous (first-order) transition for $q>4$ . This highlights how the nature of the phase transition can change with system parameters.
o _Critical Phenomena_: Near second-order transitions, critical phenomena include power-law divergences (e.g. specific heat, susceptibility), critical opalescence (fluctuations at all scales), and universality (different systems share the same critical exponents if they have the same symmetry and dimensionality). These concepts set the stage for identifying phase transitions through data features (e.g. large fluctuations near $T_c# might be detectable by learning algorithms).




!split
===== Critical Phenomena and Scaling =====

o Near a continuous transition, observables follow power laws: $M \sim |T-T_c|^\beta$, correlation length $\xi \sim |T-T_c|^{-\nu}$, etc.
o _Critical exponents_ ($\alpha,\beta,\gamma,\nu,\dots$) characterize singular behavior.
o Universality: systems with the same symmetry and dimension share exponents.
o The classical example is the  two-dimensional  Ising exponents known analytically (Onsager).
o At $T\to T_c$, correlation length $\xi\to\infty$, large-scale fluctuations appear.

!split
===== 2D Ising Model: Definition =====
o Spins $s_i = \pm 1$ on a 2D square lattice, nearest-neighbor ferromagnetic coupling.
o Hamiltonian: $H = -J \sum_{\langle i,j\rangle} s_i s_j,$ with $J>0$ favoring alignment.
o Exhibits a second-order phase transition at critical temperature $T_c$.
o Order parameter: magnetization $M = \frac{1}{N}\sum_i s_i$.
o Below $T_c$, $M\neq0$ (ferromagnetic order); above $T_c$, $M=0$ (paramagnet).

!split
===== 2D Ising Model: Critical Temperature =====
o Exact result (Onsager): critical point $T_c$ satisfies $T_c \approx \frac{2J}{\ln(1+\sqrt{2})}\approx 2.269J$.
o At $T>T_c$: spins are mostly disordered, no net magnetization.
o At $T<T_c$: long-range order develops (nonzero $M$).
o Correlation length $\xi$ diverges at $T_c$ 
o Example: at $T=T_c$ large clusters of aligned spins appear.

!split
===== q-State Potts Model: Definition =====
* Generalization of Ising: each spin $s_i \in \{1,2,\dots,q\}$.
* Ferromagnetic Potts Hamiltonian:
!bt
     \[
       H = -J \sum_{\langle i,j\rangle} \delta_{s_i,s_j},
     \]
!et     
where $\delta_{a,b}=1$ if $a=b$, else $0$.
* If $q=2$, reduces to the Ising model. Higher $q$ allows richer symmetry breaking ($\mathbb{Z}_q$).
* Widely used to study phase transitions with multiple equivalent ordered states.

!split
===== 2D Potts Model: Phase Behavior =====
* In 2D, the ferromagnetic Potts model has a phase transition for all $q\ge1$ 
* Exact critical point:
!bt
     \[
       \frac{J}{k_B T_c} = \ln\!\bigl(1+\sqrt{q}\bigr).
     \]
!et
* The nature of the transition depends on $q$ 
  * $1 \le q \le 4$: continuous (second-order) transition.
  * $q > 4$: discontinuous (first-order) transition (latent heat appears).
* Example: $q=3,4$ have continuous transitions; $q=5$ and higher show first-order behavior.

!split
===== Monte Carlo Sampling of Spin Models =====
o Use Monte Carlo (MC) to generate spin configurations at given $T$: sample from Boltzmann distribution $P\propto e^{-H/T}$.
o Metropolis algorithm: attempt random single-spin flips to equilibrate the system.
o Provides training data: spin configurations $\{s_i\}$ labeled by temperature or phase.
o Ensures statistical equilibrium and detailed balance 
o Efficient sampling (especially near $T_c$ cluster algorithms help, e.g.\ Wolff or Swendsen-Wang).

!split
===== Metropolis Algorithm =====
* Initialize spins randomly or in a fixed state.
* Repeat for many steps:
  o Pick a random lattice site $i$.
  o Propose flipping $s_i \to -s_i$ (Ising) or change state (Potts).
  o Compute energy change $\Delta E$.
  o If $\Delta E \le 0$, accept the flip (lower energy).
  o Else accept with probability $\exp(-\Delta E/T)$ (Boltzmann factor) 
  o Otherwise, reject and keep the old state.
* After equilibration, record configurations as samples.

!split
===== Metropolis Algorithm (Pseudo-code) =====
!bc pycod
for T in temperature_list:
   # Initialize lattice (e.g., random spins)
   config = random_configuration(Lx, Ly)
   for step in range(num_steps):
       i,j = random_site()
       dE = compute_deltaE(config, i, j)  # energy change if spin flipped
       if dE <= 0 or rand() < exp(-dE/T):
           flip_spin(config, i, j)
   record_configuration(config, T)
!ec

!split
===== Monte Carlo Data for ML =====
o Generate many spin configurations across a range of temperatures $T$.
o Label each configuration by its temperature or by phase (ordered/disordered).
o This labeled dataset is used for \emph{supervised} methods (e.g.\ CNN).
o For \emph{unsupervised} methods (PCA, VAE), labels are not used in training.
o Data augmentation: one can use symmetries (e.g.\ spin flip) to enlarge dataset.


!split
===== Principal Component Analysis (PCA) Basics =====

o PCA is an unsupervised method for dimensionality reduction.
o Finds orthogonal directions (principal components) of maximum variance in data.
o Project data onto the first few PCs to visualize structure.
o Advantages: linear, fast, and interpretable (PCs are linear combinations of features).
o Disadvantage: only captures linear correlations (may miss complex features).

!split
===== PCA for Phase Identification =====
o Apply PCA to the ensemble of spin configurations (flattened to vectors).
o The first principal component (PC1) often correlates with the order parameter (e.g.\ magnetization).
o Hu et al. (2017) found PCA distinguishes different phases and can locate critical points 
o By plotting data in the subspace of PCs, one sees separation of low-$T$ (ordered) vs high-$T$ (disordered) points.
o No labels needed: phase transitions are revealed by clustering in PC space 


!split
===== PCA Workflow for Spin Data =====

o Collect data matrix $X$ of shape (num\_samples) $\times$ (num\_features), e.g. $N\times (L\times L)$.
o Subtract the mean from each column (feature) of $X$.
o Compute covariance matrix $C = X^T X$ (or use SVD on $X$ directly).
o Obtain eigenvalues/vectors of $C$: $C = U \Lambda U^T$. Columns of $U$ are principal directions.
o Sort by eigenvalues (variance). Project $X$ onto top $k$ PCs: $X_{\rm red} = X\,U[:,1:k]$.
o Analyze $X_{\rm red}$: e.g. scatter plot PC1 vs PC2.


!split
===== PCA Example: Ising Model =====

o In the 2D Ising model, PC1 is essentially proportional to the overall magnetization.
o At $T<T_c$, configurations cluster with large positive or negative PC1 (ordered states).
o At $T>T_c$, configurations cluster near $PC1 \approx 0$ (disordered).
o The variance captured by PC1 drops sharply at $T_c$, signaling the transition.
o PCA automatically finds these features, without knowing the physics a priori.

!split
===== PCA Limitations =====
o PCA is linear: complex nonlinear features (e.g.\ vortex order) may not be captured.
o Example: In a frustrated 2D spin model, PCA failed to detect certain correlations (vorticity) 
o PCA does not directly classify; it provides features for clustering or visualization.
o Sensitive to scaling: data should be normalized appropriately.
o Still useful as a first-pass: identifies the most significant variations 

!split
===== PCA with PyTorch (Example Code) =====
!bc pycod
import torch

# X: tensor of shape (N, L*L) containing spin configurations as floats (e.g. +1/-1)
# Center the data
X = X - X.mean(dim=0, keepdim=True)

# Compute covariance (or use torch.pca_lowrank)
cov = torch.mm(X.t(), X) / (X.size(0)-1)

# Eigen-decomposition (SVD) of covariance
U, S, V = torch.svd(cov)

# Select first k principal components
k = 2
PCs = U[:, :k]  # shape (L*L, k)

# Project data onto principal components
X_reduced = torch.mm(X, PCs)  # shape (N, k)
!ec

!split
===== Convolutional Neural Networks (CNNs) =====

o CNNs are deep neural networks designed for spatial data (e.g. images).
o Architecture: convolutional layers (feature detectors) + pooling, followed by fully connected layers.
o In physics: treat spin lattice as an image with multiple channels (e.g.\ one channel of spins).
o CNNs can learn complex nonlinear features automatically from data.
o They require labeled examples for training (supervised learning).

!split
===== CNN for Phase Classification =====
o Prepare training data: spin configurations labeled by phase or temperature.
o CNN learns to map configuration $\to$ phase label (ordered/disordered) or predict $T$.
o As shown by Carrasquilla and Melko (2017), CNNs can identify phases from raw states 
o Achieves high accuracy on Ising and other models when training labels are available.
o CNNs exploit locality: can detect clusters or domains of aligned spins via convolution filters.

!split
===== Example CNN Architecture =====
o _Input_: single-channel $L\times L$ lattice (values $-1$ or $+1$).
o _Conv layer 1_: e.g.\ 8 filters of size $3\times3$, ReLU activation, stride=1, padding=1.
o _Conv layer 2_: 16 filters of size $3\times3$, ReLU, followed by a $2\times2$ max-pooling.
o _Fully Connected_: flatten feature maps to vector; FC layer to 64 units (ReLU); final FC to 2 outputs (softmax for binary phase).
o _Training_: minimize cross-entropy loss between predicted and true labels.
o _Note_: architecture and hyperparameters can be tuned for best performance.

!split
===== CNN: Training and Results =====
o Train on many labeled samples (e.g.\ temperatures $T$ and whether $T<T_c$ or $T>T_c$).
o The network learns features such as magnetization domains, energy patterns, etc.
o CNN accuracy can be very high (often $\sim$100\% on clean data) for distinguishing phases.
o Fukushima and Sakai (2021): a CNN trained on 2D Ising can detect transition in $q$-state Potts 
o CNN behavior: at high $T$ it effectively uses average energy; at low $T$ it correlates with magnetization 

!split
===== CNN Interpretability =====
o CNNs are often seen as _black boxes_, but their learned filters can sometimes be interpreted.
o Outputs correlate with known physics:
  o At low $T$: classification heavily influenced by magnetization (order).
  o At high $T$: classification influenced by internal energy (disorder) 
o CNNs can generalize: e.g.\ Ising-trained CNN finds Potts $T_c$ 
o Visualization methods (e.g.\ saliency maps) can highlight what CNN focuses on.

!split
===== CNN (PyTorch) Code Example =====
!bc pycod
import torch
import torch.nn as nn
import torch.nn.functional as F

class PhaseCNN(nn.Module):
   def __init__(self, L):
       super(PhaseCNN, self).__init__()
       self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)  # 1 channel -> 8
       self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1) # 8 -> 16
       self.pool = nn.MaxPool2d(2)  # downsample by 2
       self.fc1 = nn.Linear(16 * (L//2) * (L//2), 64)
       self.fc2 = nn.Linear(64, 2)  # 2 output classes

   def forward(self, x):
       x = F.relu(self.conv1(x))      # (B,8,L,L)
       x = self.pool(F.relu(self.conv2(x)))  # (B,16,L/2,L/2)
       x = x.view(x.size(0), -1)      # flatten
       x = F.relu(self.fc1(x))
       x = self.fc2(x)               # logits for 2 classes
       return x

# Example usage:
model = PhaseCNN(L=32)           # for a 32x32 lattice
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
!ec

!split
===== Variational Autoencoders (VAE) Overview =====

o A VAE is an \emph{unsupervised} generative model that learns a latent representation of data.
o Components:
   o _Encoder_: maps input $X$ to parameters $(\mu, \log\sigma^2)$ of a latent Gaussian.
   o _Latent_ $z$: sampled via $z = \mu + \sigma \epsilon$ ($\epsilon\sim N(0,I)$).
   o _Decoder_: reconstructs input $\hat{X}$ from $z$.
o Loss: reconstruction error + KL divergence to enforce latent prior $\mathcal{N}(0,I)$.
o VAEs can both encode data compactly and generate new samples by sampling $z$.

!split
===== VAE for Spin Configurations =====

o Train VAE on spin configurations (no labels).
o Latent space (usually low-dimensional) captures key features (like order parameter).
o Walker et al. (2020): latent variables provide metrics to track order vs disorder in Ising 
o They found the latent representation closely corresponds to physical order (magnetization) 
o After training, one can:
  o Inspect latent space (e.g.\ scatter plot of $(\mu_1,\mu_2)$) to distinguish phases.
  o Sample $z\sim N(0,1)$ and decode to generate synthetic configurations.

!split
===== VAE Architecture Details =====

* Typically use convolutional encoder/decoder for 2D structure.
* Example:
  o Encoder: conv layers downsampling to a flat vector $\rightarrow$ linear layers $\rightarrow (\mu, \log\sigma^2)$ (size of latent space, e.g.\ 2–10 dims).
  o Decoder: linear layer from $z$ to feature map size, followed by transposed-conv layers to reconstruct $L\times L$ lattice.
* Activation: ReLU (or LeakyReLU); final output often sigmoid to model spin distribution.
* Training with minibatch gradient descent optimizing
!bt
     \[
       \mathcal{L} = \mathbb{E}[\|X - \hat{X}\|^2] +
       \mathrm{KL}(\mathcal{N}(\mu,\sigma)\,\|\,\mathcal{N}(0,1)).
     \]
!et

!split
===== VAE Results on Ising Model =====

o The first latent dimension ($\nu_0$) learned by the VAE correlated strongly with magnetization 
o Plotting $\nu_0$ vs temperature shows clear change around $T_c$ (order–disorder).
o This means VAE "discovered" the order parameter without supervision.
o The VAE predicted the critical region and crossover consistently with theory 
o Latent space clustering: ordered-phase points separate from disordered.


!split
===== VAE: Generation and Interpretation =====

o After training, sample random $z$ from Gaussian prior and decode to generate configurations.
o The VAE latent space is continuous: can interpolate between phases.
o The learned representation is smooth and disentangled: one latent coordinate tracks magnetization, others track disorder.
o VAEs can also be used for anomaly detection: points with unusual $z$ indicate atypical states.
o Overall, VAEs provide both a dimensionally-reduced view of phase structure and a generative model.

!split
===== VAE (PyTorch) Code Example =====
!bc pycod
import torch
import torch.nn as nn
import torch.nn.functional as F

class VAE(nn.Module):
   def __init__(self, L, latent_dim=2):
       super(VAE, self).__init__()
       # Encoder: conv -> conv -> flatten -> fc_mu/fc_logvar
       self.encoder = nn.Sequential(
           nn.Conv2d(1, 8, 3, stride=2, padding=1),   # -> (8, L/2, L/2)
           nn.ReLU(),
           nn.Conv2d(8, 16, 3, stride=2, padding=1),  # -> (16, L/4, L/4)
           nn.ReLU(),
           nn.Flatten()
       )
       self.fc_mu = nn.Linear(16*(L//4)*(L//4), latent_dim)
       self.fc_logvar = nn.Linear(16*(L//4)*(L//4), latent_dim)

       # Decoder: linear -> unflatten -> convTranspose -> convTranspose
       self.decoder_fc = nn.Linear(latent_dim, 16*(L//4)*(L//4))
       self.decoder = nn.Sequential(
           nn.Unflatten(1, (16, L//4, L//4)),
           nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),  # -> (8, L/2, L/2)
           nn.ReLU(),
           nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1),   # -> (1, L, L)
           nn.Sigmoid()
       )

   def encode(self, x):
       h = self.encoder(x)
       mu = self.fc_mu(h)
       logvar = self.fc_logvar(h)
       return mu, logvar

   def reparameterize(self, mu, logvar):
       std = torch.exp(0.5*logvar)
       eps = torch.randn_like(std)
       return mu + eps * std

   def decode(self, z):
       h = self.decoder_fc(z)
       x_recon = self.decoder(h)
       return x_recon

   def forward(self, x):
       mu, logvar = self.encode(x)
       z = self.reparameterize(mu, logvar)
       x_recon = self.decode(z)
       return x_recon, mu, logvar

# Example instantiation:
model = VAE(L=32, latent_dim=2)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
!ec

!split
===== Supervised vs Unsupervised Methods =====

o _Supervised (CNN)_: Requires labeled data (phase labels or temperatures). Learns a direct mapping $\{\text{config}\}\to\{\text{phase}\}$.
o _Unsupervised (PCA, VAE)_: Uses only the raw configurations without labels. Learns features or representations of the data.
o PCA reduces dimensionality; requires no training labels 
o VAE learns a latent generative model; also label-free 
o CNN typically achieves higher accuracy in classifying known phases, but needs supervised labels.

!split
===== Method Interpretability and Features =====

o _PCA_: Principal components often have clear physical meaning (e.g.\ PC1 and  magnetization) 
o _CNN_: Filters are less directly interpretable; features are learned. However, some correlation with physics (energy, $M$) emerges 
o _VAE_: Latent variables can often be interpreted as order/disorder features (e.g. one latent is approximately equal to the  magnetization) 
o CNN is a "black box" classifier; PCA/VAE provide insight into data structure.
o In terms of visualization: PCA and VAE produce low-dim plots of data (semi-transparent), whereas CNN only outputs a decision boundary.

!split
===== Performance and Use Cases =====

o _PCA_: Fast to compute; good for preliminary analysis of large datasets. Best for linearizable transitions.
o _CNN_: High classification accuracy; powerful for large and complex datasets. Can predict critical $T$ or classify multiple phases 
o _VAE_: Useful when no labels are available; provides a generative model. Effective in detecting transitions by latent statistics 
o Computational cost: PCA very cheap, CNN and VAE require training time (GPU recommended for large data).
o Choosing a method: depends on data availability and goal (classification vs insight vs generation).

!split
===== Summary of Methods =====
o _PCA_: Unsupervised, linear, interpretable. Good for dimensionality reduction and initial exploration
o _CNN_: Supervised, non-linear, high accuracy. Requires labels, but learns complex features (works across models 
o _VAE_: Unsupervised, generative. Learns latent representation reflecting order/disorder 
o Each method has trade-offs in accuracy, interpretability, and data requirements.
o Combining methods (e.g.\ using PCA or VAE features as input to another classifier) can also be fruitful.

!split
===== Conclusions =====
o Machine learning provides powerful tools for studying phase transitions in statistical models.
o *Unsupervised* methods (PCA, VAE) can discover phase structure without labels 
o *Supervised* methods (CNNs) achieve high classification performance given labeled data 
o Interpretability: PCA/VAE offer more insight into physics (latent/PC represent order parameters), while CNNs focus on prediction accuracy.
o Choice of method depends on the problem: data availability, need for generative modeling, and interpretability.
o Future directions: deeper architectures (e.g.\ ResNets), unsupervised generative flows, transfer learning across models, real experimental data.

!split
===== References =====
o Carrasquilla, J. \& Melko, R. G. (2017). Machine learning phases of matter. \emph{Nature Physics}, 13, 431–434 
o Hu, W. \textit{et al.} (2017). Discovering phases, phase transitions through unsupervised ML. \emph{Phys. Rev. E} 95, 062122 
o Fukushima, K. \& Sakai, K. (2021). Can a CNN trained on Ising detect Potts? \emph{Prog. Theor. Exp. Phys.} 2021, 061A01 
o Walker, N. \textit{et al.} (2020). 2D Ising model crossover via VAE. \emph{Sci. Rep.} 10, 13047
o Add refs







!split
===== 2D Ising Model (Ferromagnet) =====





Figure: Two-dimensional Ising model with spins up (pink arrows) and down (blue arrows) on a square lattice. Each spin can be $s_i = \pm 1$ and interacts with its nearest neighbors; aligned neighbors (same color) contribute a lower energy ($-J$ per bond) while anti-aligned neighbors contribute higher energy ($+J$). The Hamiltonian (with no external field) is $H = -J \sum_{\langle i,j\rangle} s_i s_j$, favoring parallel alignment of spins. The competition between interaction energy and thermal agitation leads to a ferromagnetic phase at low temperatures (most spins align, yielding non-zero net magnetization) and a paramagnetic phase at high temperatures (spins are disordered, zero net magnetization). The 2D square-lattice Ising model is the simplest model that exhibits a phase transition at a finite critical temperature T_c (approximately $T_c \approx 2.269,J/k_B$ for the infinite lattice). Below $T_c$ the system spontaneously magnetizes (symmetry breaking), while above $T_c$ it remains disordered.





!split
===== q-State Potts Model =====





Definition: The q-state Potts model is a generalization of the Ising model where each spin can take $q$ discrete values (e.g. $q$ “colors” instead of just up/down). The Hamiltonian for the ferromagnetic Potts model can be written $H = -J \sum_{\langle i,j\rangle} \delta_{\sigma_i,\sigma_j}$, where $\sigma_i \in {1,\dots,q}$ and $\delta$ is the Kronecker delta (neighbor interaction is $-J$ if spins are in the same state). For $q=2$, this reduces to the Ising model.
Phases: Like the Ising case, at low temperature a Potts model magnetically orders (most spins in the same state), and at high temperature it is disordered (spins randomly distributed among the $q$ states). The nature of the transition, however, depends on $q$. In 2D, all $q\ge 1$ have a phase transition at some critical temperature given by $k_B T_c/J = 1/\ln(1+\sqrt{q},)$ . For $q \le 4$ the transition is continuous (second-order), in the same universality class as the Ising model for $q=2$, whereas for $q > 4$ the transition becomes first-order (discontinuous jump in order parameter) .
Significance: The $q$-state Potts model exemplifies how increasing internal symmetry (more spin states) can change transition order. It provides a wider test-bed for machine learning methods: e.g. can an algorithm trained on one type of transition detect another? We will focus primarily on the Ising case (as $q=2$) for data generation and learning, with the understanding that methods can be extended to Potts models and beyond.






